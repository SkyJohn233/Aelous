the evolution of database schemas using sql nosql java code geeks 2019 posted by jim scott in software development july 9th 2015 0 264 views in part one of this series drilling into healthy choices we explored using drill to create parquet tables as well as configuring drill to read data formats that are not very standard we also explored the usda national nutrient database by writing some different queries in drill in part two of this series we are going to utilize this same database to think beyond traditional database design for the last 30 years software applications have leveraged rdbms for persisting their data it has been this way for a while because rdbms is generally supported by it organizations which makes it that much easier to get software into a production environment typically engineers create data structures in the programming language they are utilizing and then figure out how to map those data structures to a series of database tables within the world of java there have been many different solutions for persistence the most recent have been the java persistence api and hibernate these were meant to deliver an abstraction from the underlying database technology to offer some level of portability the main problem is that the portability was really only from one rdbms to another the secondary problem is that creating those mappings was not an easy task and it required a lot of work to test that they were working as intended it seems like it would make a lot more sense to persist the entire data structure as is this would make it truly portable as well as easier to develop and test let us not forget that it would also make it a lot easier to query the data it is kind of a nightmare to write sql queries against complicated database schemas not only can sql queries against large schemas be difficult to write but they also presents a barrier for people researching data stored within those complicated database schemas researchers have to figure out which columns in which tables mean what within the confines of the questions for which they seek answers things could be easier for everyone involved at every stage of the process let s revisit the database schema from the last article covering the national nutrient database provided by the usda starting at the top of this schema we can see that the fd_group table is nothing more than a detail table that yields a maximum of one record per entry in the food_des table we also see these same relationships with the src_cd nutr_def and deriv_cd onto the nut_data table the nut_data footnote and weight tables each can have zero or more entries per item in the food_des table and yield what i like to call lists the datsrcln table is nothing more than a join table from nut_data to data_src and the same goes for the langual table to the langdesc table both generate nothing more than lists of data this table structure is built the way it is in order to support the concepts of lists of either simple or complex data the benefit that previously existed from this type of table schema was that it saved money on disk storage storage costs are now thousands of times less than they were 10 to 30 years ago so we will not really be saving any money on storage let s start simplifying the database schema to see if we can make it better for all parties involved take a look at the changes in the food_des table the fdgrp_desc from the fd_group table has been merged into the food_des table the weight table gets merged into the food_des table as lists this is a list because there could be zero or more elements in the tables relating to the food_des table we can distill the langual and langdesc table down to a list of descriptions from the langdesc table and store them in the food_des table the nut_data table can become a list of elements in the food-des table we can then take the src_cd nutr_def and deriv_cd and just merge them directly into the nut_data table structure the datasrcln table is a join table for data_src back to nut_data so we just merge that back as a list of elements as well the more interesting case here is that after inspecting the footnote table from the original schema it seems that the relationship between footnote and nut_data was not written properly the arrow should probably point in the opposite direction the footnote table contains two types of footnotes it contains footnotes related to the food_des via the ndb_no but it also relates to the nut_data via a combination of ndb_no and nutr_no to support this in the new schema we will break up the footnote table and create a list of footnotes in nut_data and food_des the tables that are now gray are json formatted data structures within another table the white tables are basically just merged into their parent tables either directly or as a list by wasting a little bit of space compared to the standard design we now end up with a table of data that is suddenly now capable of being understood by anyone here is a simpler view of this data schema the table on the left is the food_des table and the table on the right is just the json data structure that formerly encompassed the nutrition data nut_data as you look through this you will notice that it contains all the fields that were originally defined in the tables minus some of the fields that are no longer relevant like sequence numbers or factor codes or any other field that was just being used to create a relationship between tables here is a data extract of some data from the original database and in this json structure for you to test with this data comprises one food_des record i manually added a langual entry which did not exist in the original database for this record copy this data and put it into a file /tmp/food_des.json and you will be able to run the rest of the queries on your own apache drill was built to be able to query complex data structures like the table presented above we can start with one of the simplest use cases which is the list lt langual that is in the table that list is a common way of describing the food_des item in order to find the common language information for a single item in the original schema we would have to run this query this is the query in the new schema which utilizes the flatten function within drill both of these queries return one row for each description in the langual list the second query is much easier to understand and to write this translates directly into time savings for anyone wanting to perform research as they will have a much easier time working with the data let s look at a query that is considerably more complex we are going to select some fields from multiple tables to find out details about a food item and the source of those details this query is not all inclusive of all the details that could be pulled in and is limited to three nutr_no records here is what the same query would like against the new schema while the two queries look similar in length the complexity of them is drastically different the second query only looks long because of all the naming of the fields in the select statement changing that query to select would leave us with from this view it becomes very easy to see that we are not really doing much beyond a couple of nested sql statements to flatten those nested lists of data this doesn t really require subject matter expertise like joining the complex set of tables and by all measures this national nutrient database weighing in at 12 tables is rather simple compared to most databases that i have worked on which easily measure into the hundreds of tables there are two other functions which are very convenient for inspecting nested data the first is repeated_count as you might guess this function counts the number of items in a list with this query we can see how many elements are in the langual list the next function is repeated_contains this function checks a list for a keyword to see if it exists here we will check to see if this item is described with the word fruit you could also change this to any arbitrary text to verify that it returns false these two functions are very easy to work with the repeated_contains is great in a where clause to limit the result sets while performing research for example in the case above limiting the query to only include items listed as fruit so far we have learned how to simplify our database schema to make it so that anyone can access the data with minimal understanding of the database we have also learned how to use this sql query engine to query very complex json structures now we need to look into where we can persist our data from an application standpoint persisting serializing a data structure in json format within a language like java is very simple and only takes a couple lines of code no complex table mapping required we now can prevent all the additional work of figuring out how to map yourobject.java data structure to a set of tables in a database schema this will save you a lot of time and pain now you can reap the benefits of a nosql store like hbase you can use it for online transactional processing oltp to enable linear scalability at the database level for any application this means that within those enterprise applications there is no longer a reason to worry about how to scale the platform when hitting performance limits like in a traditional rdbms system just add a server to the cluster and you are good to scale you can also perform real-time analytics on the json data directly against hbase using drill because drill has the ability to query hbase out of the box there is no need to do a data extract or to transform the data into a different format i hope this article has left you inspired to think outside of the traditional approaches of database design and database systems please share your thoughts or ideas on these topics below i would love to hear from you tagged with databases nosql sql this site uses akismet to reduce spam learn how your comment data is processed