{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c2db82288fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menglish_punctuations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     return [\n\u001b[1;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "documents = []\n",
    "count = 0\n",
    "input_dir = './processed_html/'\n",
    "output_dir = './tokenized_html/'\n",
    "english_punctuations = ['|', '-', 'Â·', '``', ',', '.', ':', ';', '?', '(', ')', '[', ']', '<', '>', '&', '!', '*', '@', '#', '$', '%', '...', \"'\", '\"', \"--\", \"`\", \"â\", \"â\", \"â\", \"â\", \"_\", \"''\", '+', '*', '/', '\\\\', '=', '~', '^', '{', '}']\n",
    "count = 0\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    with open(os.path.join(input_dir, file), 'r', encoding='utf-8') as f:\n",
    "        lines = []\n",
    "        for line in f.readlines():\n",
    "            line = nltk.word_tokenize(line)\n",
    "            line = [word.lower() for word in line if word not in english_punctuations]\n",
    "            lines.extend(line)\n",
    "        rec = open(os.path.join(output_dir, file), 'w', encoding='utf-8')\n",
    "        flag = False\n",
    "        for word in lines:\n",
    "            if flag:\n",
    "                rec.write(' ' + word)\n",
    "            else:\n",
    "                rec.write(word)\n",
    "                flag = True\n",
    "        rec.close()\n",
    "        documents.append(gensim.models.doc2vec.TaggedDocument(lines, [str(count)]))\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            logging.info('{} has loaded...'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/usr/local/lib/python3.5/dist-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2020-01-15 06:59:26,203 : INFO : collecting all words and their counts\n",
      "2020-01-15 06:59:26,205 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-01-15 06:59:28,301 : INFO : PROGRESS: at example #10000, processed 7657892 words (3655466/s), 385705 word types, 10000 tags\n",
      "2020-01-15 06:59:28,447 : INFO : collected 405064 word types and 10829 unique tags from a corpus of 10829 examples and 8191789 words\n",
      "2020-01-15 06:59:28,448 : INFO : Loading a fresh vocabulary\n",
      "2020-01-15 06:59:29,297 : INFO : effective_min_count=5 retains 89547 unique words (22% of original 405064, drops 315517)\n",
      "2020-01-15 06:59:29,298 : INFO : effective_min_count=5 leaves 7701274 word corpus (94% of original 8191789, drops 490515)\n",
      "2020-01-15 06:59:29,596 : INFO : deleting the raw counts dictionary of 405064 items\n",
      "2020-01-15 06:59:29,620 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2020-01-15 06:59:29,621 : INFO : downsampling leaves estimated 6743911 word corpus (87.6% of prior 7701274)\n",
      "2020-01-15 06:59:30,032 : INFO : estimated required memory for 89547 words and 100 dimensions: 122908500 bytes\n",
      "2020-01-15 06:59:30,033 : INFO : resetting layer weights\n",
      "2020-01-15 06:59:31,267 : INFO : training model with 4 workers on 89547 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=8\n",
      "2020-01-15 06:59:32,274 : INFO : EPOCH 1 - PROGRESS: at 16.54% examples, 944878 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:33,287 : INFO : EPOCH 1 - PROGRESS: at 33.25% examples, 984259 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:34,294 : INFO : EPOCH 1 - PROGRESS: at 50.00% examples, 987467 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:35,297 : INFO : EPOCH 1 - PROGRESS: at 66.50% examples, 975589 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:36,306 : INFO : EPOCH 1 - PROGRESS: at 81.97% examples, 973897 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 06:59:37,307 : INFO : EPOCH 1 - PROGRESS: at 97.47% examples, 972931 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:37,414 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 06:59:37,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 06:59:37,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 06:59:37,435 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 06:59:37,436 : INFO : EPOCH - 1 : training on 8191789 raw words (6004022 effective words) took 6.2s, 973985 effective words/s\n",
      "2020-01-15 06:59:38,445 : INFO : EPOCH 2 - PROGRESS: at 16.34% examples, 926938 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:39,449 : INFO : EPOCH 2 - PROGRESS: at 31.68% examples, 950706 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:40,453 : INFO : EPOCH 2 - PROGRESS: at 47.70% examples, 950133 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:41,453 : INFO : EPOCH 2 - PROGRESS: at 65.44% examples, 958186 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:42,455 : INFO : EPOCH 2 - PROGRESS: at 80.36% examples, 957309 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:43,462 : INFO : EPOCH 2 - PROGRESS: at 96.01% examples, 961609 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:43,668 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 06:59:43,676 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 06:59:43,681 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 06:59:43,687 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 06:59:43,687 : INFO : EPOCH - 2 : training on 8191789 raw words (6004839 effective words) took 6.2s, 961177 effective words/s\n",
      "2020-01-15 06:59:44,701 : INFO : EPOCH 3 - PROGRESS: at 16.76% examples, 962557 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:45,705 : INFO : EPOCH 3 - PROGRESS: at 32.28% examples, 964863 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:46,708 : INFO : EPOCH 3 - PROGRESS: at 48.84% examples, 971899 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:47,713 : INFO : EPOCH 3 - PROGRESS: at 65.91% examples, 966632 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:48,715 : INFO : EPOCH 3 - PROGRESS: at 80.96% examples, 965392 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 06:59:49,719 : INFO : EPOCH 3 - PROGRESS: at 96.76% examples, 969256 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:49,863 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 06:59:49,873 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 06:59:49,876 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 06:59:49,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 06:59:49,881 : INFO : EPOCH - 3 : training on 8191789 raw words (6004239 effective words) took 6.2s, 970445 effective words/s\n",
      "2020-01-15 06:59:50,893 : INFO : EPOCH 4 - PROGRESS: at 16.71% examples, 952265 words/s, in_qsize 7, out_qsize 1\n",
      "2020-01-15 06:59:51,905 : INFO : EPOCH 4 - PROGRESS: at 33.81% examples, 996512 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:52,907 : INFO : EPOCH 4 - PROGRESS: at 50.84% examples, 1000998 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:53,908 : INFO : EPOCH 4 - PROGRESS: at 67.83% examples, 990818 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:54,925 : INFO : EPOCH 4 - PROGRESS: at 83.63% examples, 988477 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:55,933 : INFO : EPOCH 4 - PROGRESS: at 99.11% examples, 986302 words/s, in_qsize 6, out_qsize 0\n",
      "2020-01-15 06:59:55,947 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 06:59:55,955 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 06:59:55,960 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 06:59:55,965 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 06:59:55,966 : INFO : EPOCH - 4 : training on 8191789 raw words (6003708 effective words) took 6.1s, 987206 effective words/s\n",
      "2020-01-15 06:59:56,974 : INFO : EPOCH 5 - PROGRESS: at 16.80% examples, 964826 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:57,981 : INFO : EPOCH 5 - PROGRESS: at 32.69% examples, 972326 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 06:59:58,986 : INFO : EPOCH 5 - PROGRESS: at 49.36% examples, 979589 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:59,993 : INFO : EPOCH 5 - PROGRESS: at 66.50% examples, 976614 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:00,998 : INFO : EPOCH 5 - PROGRESS: at 82.01% examples, 976931 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:02,014 : INFO : EPOCH 5 - PROGRESS: at 98.49% examples, 981275 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:02,068 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:00:02,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:00:02,077 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:00:02,084 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:00:02,085 : INFO : EPOCH - 5 : training on 8191789 raw words (6005168 effective words) took 6.1s, 982116 effective words/s\n",
      "2020-01-15 07:00:03,096 : INFO : EPOCH 6 - PROGRESS: at 16.61% examples, 951659 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:04,100 : INFO : EPOCH 6 - PROGRESS: at 32.44% examples, 970488 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:05,116 : INFO : EPOCH 6 - PROGRESS: at 48.84% examples, 969193 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:06,126 : INFO : EPOCH 6 - PROGRESS: at 66.16% examples, 969028 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-15 07:00:07,128 : INFO : EPOCH 6 - PROGRESS: at 81.97% examples, 973536 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:08,131 : INFO : EPOCH 6 - PROGRESS: at 97.65% examples, 975272 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:08,231 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:00:08,238 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:00:08,242 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:00:08,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:00:08,249 : INFO : EPOCH - 6 : training on 8191789 raw words (6003944 effective words) took 6.2s, 975233 effective words/s\n",
      "2020-01-15 07:00:09,260 : INFO : EPOCH 7 - PROGRESS: at 16.70% examples, 955942 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:10,262 : INFO : EPOCH 7 - PROGRESS: at 33.23% examples, 986583 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:11,267 : INFO : EPOCH 7 - PROGRESS: at 51.24% examples, 1007321 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:12,270 : INFO : EPOCH 7 - PROGRESS: at 68.47% examples, 999887 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:13,271 : INFO : EPOCH 7 - PROGRESS: at 83.70% examples, 994641 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:14,247 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:00:14,254 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:00:14,255 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:00:14,265 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:00:14,266 : INFO : EPOCH - 7 : training on 8191789 raw words (6004605 effective words) took 6.0s, 998585 effective words/s\n",
      "2020-01-15 07:00:15,277 : INFO : EPOCH 8 - PROGRESS: at 16.88% examples, 976188 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:16,288 : INFO : EPOCH 8 - PROGRESS: at 33.23% examples, 983994 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:17,289 : INFO : EPOCH 8 - PROGRESS: at 50.00% examples, 989098 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:18,291 : INFO : EPOCH 8 - PROGRESS: at 67.19% examples, 984858 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:19,296 : INFO : EPOCH 8 - PROGRESS: at 83.02% examples, 984916 words/s, in_qsize 6, out_qsize 1\n",
      "2020-01-15 07:00:20,309 : INFO : EPOCH 8 - PROGRESS: at 99.01% examples, 987367 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:20,334 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:00:20,340 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:00:20,341 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:00:20,349 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:00:20,350 : INFO : EPOCH - 8 : training on 8191789 raw words (6005037 effective words) took 6.1s, 987981 effective words/s\n",
      "2020-01-15 07:00:21,364 : INFO : EPOCH 9 - PROGRESS: at 17.07% examples, 985931 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:22,364 : INFO : EPOCH 9 - PROGRESS: at 33.11% examples, 982824 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:23,370 : INFO : EPOCH 9 - PROGRESS: at 49.36% examples, 978922 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:24,371 : INFO : EPOCH 9 - PROGRESS: at 66.85% examples, 979684 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:25,377 : INFO : EPOCH 9 - PROGRESS: at 82.01% examples, 977642 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:26,390 : INFO : EPOCH 9 - PROGRESS: at 98.49% examples, 982408 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:26,439 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:00:26,444 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:00:26,449 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:00:26,462 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:00:26,463 : INFO : EPOCH - 9 : training on 8191789 raw words (6005157 effective words) took 6.1s, 982813 effective words/s\n",
      "2020-01-15 07:00:27,471 : INFO : EPOCH 10 - PROGRESS: at 16.61% examples, 950783 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:28,472 : INFO : EPOCH 10 - PROGRESS: at 32.69% examples, 975015 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:29,473 : INFO : EPOCH 10 - PROGRESS: at 48.84% examples, 974647 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:30,473 : INFO : EPOCH 10 - PROGRESS: at 66.85% examples, 982457 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:31,475 : INFO : EPOCH 10 - PROGRESS: at 82.33% examples, 982272 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:32,480 : INFO : EPOCH 10 - PROGRESS: at 97.64% examples, 977918 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 07:00:32,585 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:00:32,593 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:00:32,594 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:00:32,602 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:00:32,602 : INFO : EPOCH - 10 : training on 8191789 raw words (6005529 effective words) took 6.1s, 978752 effective words/s\n",
      "2020-01-15 07:00:32,603 : INFO : training on a 81917890 raw words (60046248 effective words) took 61.3s, 978995 effective words/s\n",
      "2020-01-15 07:00:32,604 : INFO : saving Doc2Vec object under models/doc2vec1, separately None\n",
      "2020-01-15 07:00:33,744 : INFO : saved models/doc2vec1\n"
     ]
    }
   ],
   "source": [
    "# åæ°å¯è°\n",
    "model = Doc2Vec(documents, dm=1, size=100, window=8, min_count=5, workers=4, iter=10)\n",
    "model.save('models/doc2vec1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/usr/local/lib/python3.5/dist-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2020-01-15 07:02:36,085 : INFO : collecting all words and their counts\n",
      "2020-01-15 07:02:36,086 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-01-15 07:02:37,839 : INFO : PROGRESS: at example #10000, processed 7657892 words (4372135/s), 385705 word types, 10000 tags\n",
      "2020-01-15 07:02:37,965 : INFO : collected 405064 word types and 10829 unique tags from a corpus of 10829 examples and 8191789 words\n",
      "2020-01-15 07:02:37,966 : INFO : Loading a fresh vocabulary\n",
      "2020-01-15 07:02:39,420 : INFO : effective_min_count=2 retains 198175 unique words (48% of original 405064, drops 206889)\n",
      "2020-01-15 07:02:39,421 : INFO : effective_min_count=2 leaves 7984900 word corpus (97% of original 8191789, drops 206889)\n",
      "2020-01-15 07:02:40,073 : INFO : deleting the raw counts dictionary of 405064 items\n",
      "2020-01-15 07:02:40,094 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2020-01-15 07:02:40,095 : INFO : downsampling leaves estimated 7049636 word corpus (88.3% of prior 7984900)\n",
      "2020-01-15 07:02:41,029 : INFO : estimated required memory for 198175 words and 100 dimensions: 264124900 bytes\n",
      "2020-01-15 07:02:41,030 : INFO : resetting layer weights\n",
      "2020-01-15 07:02:43,693 : INFO : training model with 4 workers on 198175 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=8\n",
      "2020-01-15 07:02:44,704 : INFO : EPOCH 1 - PROGRESS: at 13.69% examples, 781341 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:02:45,705 : INFO : EPOCH 1 - PROGRESS: at 27.11% examples, 857536 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:02:46,714 : INFO : EPOCH 1 - PROGRESS: at 42.24% examples, 871471 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:02:47,724 : INFO : EPOCH 1 - PROGRESS: at 58.37% examples, 892527 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:02:48,742 : INFO : EPOCH 1 - PROGRESS: at 73.79% examples, 897581 words/s, in_qsize 7, out_qsize 1\n",
      "2020-01-15 07:02:49,744 : INFO : EPOCH 1 - PROGRESS: at 86.81% examples, 899299 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:02:50,609 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:02:50,622 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:02:50,625 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:02:50,634 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:02:50,635 : INFO : EPOCH - 1 : training on 8191789 raw words (6265210 effective words) took 6.9s, 903234 effective words/s\n",
      "2020-01-15 07:02:51,649 : INFO : EPOCH 2 - PROGRESS: at 15.05% examples, 876604 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:02:52,656 : INFO : EPOCH 2 - PROGRESS: at 28.80% examples, 908042 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:02:53,658 : INFO : EPOCH 2 - PROGRESS: at 43.60% examples, 896490 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:02:54,666 : INFO : EPOCH 2 - PROGRESS: at 58.81% examples, 900258 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:02:55,672 : INFO : EPOCH 2 - PROGRESS: at 74.06% examples, 902531 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:02:56,683 : INFO : EPOCH 2 - PROGRESS: at 87.63% examples, 905868 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:02:57,493 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:02:57,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:02:57,506 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:02:57,519 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:02:57,520 : INFO : EPOCH - 2 : training on 8191789 raw words (6264114 effective words) took 6.9s, 910250 effective words/s\n",
      "2020-01-15 07:02:58,535 : INFO : EPOCH 3 - PROGRESS: at 15.05% examples, 875107 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:02:59,540 : INFO : EPOCH 3 - PROGRESS: at 28.80% examples, 908232 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:00,541 : INFO : EPOCH 3 - PROGRESS: at 43.75% examples, 905842 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:01,549 : INFO : EPOCH 3 - PROGRESS: at 59.22% examples, 909417 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:02,552 : INFO : EPOCH 3 - PROGRESS: at 74.36% examples, 909151 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:03,562 : INFO : EPOCH 3 - PROGRESS: at 88.12% examples, 912751 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:04,342 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:04,353 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:04,355 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:04,366 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:04,367 : INFO : EPOCH - 3 : training on 8191789 raw words (6263225 effective words) took 6.8s, 915234 effective words/s\n",
      "2020-01-15 07:03:05,376 : INFO : EPOCH 4 - PROGRESS: at 15.35% examples, 899194 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:06,388 : INFO : EPOCH 4 - PROGRESS: at 29.63% examples, 931082 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:07,390 : INFO : EPOCH 4 - PROGRESS: at 45.06% examples, 935144 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:08,401 : INFO : EPOCH 4 - PROGRESS: at 60.45% examples, 932562 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:09,407 : INFO : EPOCH 4 - PROGRESS: at 75.56% examples, 926544 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:10,423 : INFO : EPOCH 4 - PROGRESS: at 89.49% examples, 928428 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:11,114 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:11,122 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:11,124 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:11,135 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:11,136 : INFO : EPOCH - 4 : training on 8191789 raw words (6263688 effective words) took 6.8s, 926481 effective words/s\n",
      "2020-01-15 07:03:12,151 : INFO : EPOCH 5 - PROGRESS: at 15.74% examples, 927105 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:13,154 : INFO : EPOCH 5 - PROGRESS: at 29.49% examples, 928880 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:14,156 : INFO : EPOCH 5 - PROGRESS: at 44.25% examples, 917135 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:15,167 : INFO : EPOCH 5 - PROGRESS: at 59.75% examples, 917595 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 07:03:16,173 : INFO : EPOCH 5 - PROGRESS: at 74.71% examples, 912585 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:17,174 : INFO : EPOCH 5 - PROGRESS: at 88.50% examples, 917047 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:17,968 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:17,975 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:17,978 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:17,989 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:17,989 : INFO : EPOCH - 5 : training on 8191789 raw words (6263903 effective words) took 6.8s, 915321 effective words/s\n",
      "2020-01-15 07:03:18,999 : INFO : EPOCH 6 - PROGRESS: at 15.44% examples, 906015 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:20,006 : INFO : EPOCH 6 - PROGRESS: at 28.80% examples, 911884 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:21,008 : INFO : EPOCH 6 - PROGRESS: at 43.89% examples, 910502 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:22,014 : INFO : EPOCH 6 - PROGRESS: at 59.50% examples, 915457 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-15 07:03:23,018 : INFO : EPOCH 6 - PROGRESS: at 74.34% examples, 907773 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 07:03:24,023 : INFO : EPOCH 6 - PROGRESS: at 88.06% examples, 913226 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:24,826 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:24,832 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:24,839 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:24,850 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:24,851 : INFO : EPOCH - 6 : training on 8191789 raw words (6264357 effective words) took 6.9s, 913949 effective words/s\n",
      "2020-01-15 07:03:25,856 : INFO : EPOCH 7 - PROGRESS: at 15.68% examples, 922370 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:26,864 : INFO : EPOCH 7 - PROGRESS: at 29.87% examples, 940302 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:27,871 : INFO : EPOCH 7 - PROGRESS: at 45.45% examples, 940033 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:28,890 : INFO : EPOCH 7 - PROGRESS: at 60.45% examples, 930553 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:29,896 : INFO : EPOCH 7 - PROGRESS: at 75.66% examples, 926281 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:30,910 : INFO : EPOCH 7 - PROGRESS: at 89.49% examples, 927490 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:31,608 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:31,613 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:31,621 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:31,633 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:31,634 : INFO : EPOCH - 7 : training on 8191789 raw words (6264825 effective words) took 6.8s, 924047 effective words/s\n",
      "2020-01-15 07:03:32,653 : INFO : EPOCH 8 - PROGRESS: at 15.51% examples, 901860 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:33,657 : INFO : EPOCH 8 - PROGRESS: at 28.97% examples, 911219 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:34,657 : INFO : EPOCH 8 - PROGRESS: at 44.25% examples, 914393 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:35,663 : INFO : EPOCH 8 - PROGRESS: at 59.22% examples, 909512 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:36,667 : INFO : EPOCH 8 - PROGRESS: at 74.85% examples, 915525 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:37,668 : INFO : EPOCH 8 - PROGRESS: at 88.13% examples, 914309 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:38,455 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:38,466 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:38,467 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:38,481 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:38,481 : INFO : EPOCH - 8 : training on 8191789 raw words (6264942 effective words) took 6.8s, 915415 effective words/s\n",
      "2020-01-15 07:03:39,499 : INFO : EPOCH 9 - PROGRESS: at 15.13% examples, 870782 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 07:03:40,500 : INFO : EPOCH 9 - PROGRESS: at 28.56% examples, 905085 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:41,501 : INFO : EPOCH 9 - PROGRESS: at 44.04% examples, 911243 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:42,510 : INFO : EPOCH 9 - PROGRESS: at 59.38% examples, 911484 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:43,511 : INFO : EPOCH 9 - PROGRESS: at 74.71% examples, 912650 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:44,519 : INFO : EPOCH 9 - PROGRESS: at 88.39% examples, 914999 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:45,254 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:45,263 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:45,264 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:45,277 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:45,278 : INFO : EPOCH - 9 : training on 8191789 raw words (6264276 effective words) took 6.8s, 922152 effective words/s\n",
      "2020-01-15 07:03:46,288 : INFO : EPOCH 10 - PROGRESS: at 15.92% examples, 932257 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:47,289 : INFO : EPOCH 10 - PROGRESS: at 30.53% examples, 956315 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:48,289 : INFO : EPOCH 10 - PROGRESS: at 45.06% examples, 937348 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:49,293 : INFO : EPOCH 10 - PROGRESS: at 60.39% examples, 933874 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:50,297 : INFO : EPOCH 10 - PROGRESS: at 75.55% examples, 927548 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:51,302 : INFO : EPOCH 10 - PROGRESS: at 89.13% examples, 927965 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:51,991 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:52,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:52,004 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:52,016 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:52,017 : INFO : EPOCH - 10 : training on 8191789 raw words (6264012 effective words) took 6.7s, 929973 effective words/s\n",
      "2020-01-15 07:03:52,017 : INFO : training on a 81917890 raw words (62642552 effective words) took 68.3s, 916865 effective words/s\n",
      "2020-01-15 07:03:52,074 : INFO : saving Doc2Vec object under models/doc2vec2, separately None\n",
      "2020-01-15 07:03:52,075 : INFO : storing np array 'vectors' to models/doc2vec2.wv.vectors.npy\n",
      "2020-01-15 07:03:52,133 : INFO : storing np array 'syn1neg' to models/doc2vec2.trainables.syn1neg.npy\n",
      "2020-01-15 07:03:52,847 : INFO : saved models/doc2vec2\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(documents, dm=1, size=100, window=8, min_count=2, workers=4, iter=10)\n",
    "model.save('models/doc2vec2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-15 07:21:27,023 : INFO : loading Doc2Vec object from models/doc2vec1\n",
      "2020-01-15 07:21:27,911 : INFO : loading wv recursively from models/doc2vec1.wv.* with mmap=None\n",
      "2020-01-15 07:21:27,915 : INFO : loading docvecs recursively from models/doc2vec1.docvecs.* with mmap=None\n",
      "2020-01-15 07:21:27,916 : INFO : loading vocabulary recursively from models/doc2vec1.vocabulary.* with mmap=None\n",
      "2020-01-15 07:21:27,917 : INFO : loading trainables recursively from models/doc2vec1.trainables.* with mmap=None\n",
      "2020-01-15 07:21:27,918 : INFO : loaded models/doc2vec1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89547 word vectors.\n",
      "89548 100\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å¥é¢è®­ç»çè¯åé\n",
    "model_doc2vec = Doc2Vec.load('models/doc2vec1')\n",
    "word2idx = {\"_PAD\": 0}  # åå§å `[word : index]` å­å¸\n",
    "vocab_list = [(k, model_doc2vec.wv[k]) for k, v in model_doc2vec.wv.vocab.items()]\n",
    "# è¯åéç©éµ\n",
    "embeddings_matrix = np.zeros((len(model_doc2vec.wv.vocab.items()) + 1, model_doc2vec.vector_size))\n",
    "print('Found %s word vectors.' % len(model_doc2vec.wv.vocab.items()))\n",
    "for i in range(len(vocab_list)):\n",
    "\tword = vocab_list[i][0]\n",
    "\tword2idx[word] = i + 1\n",
    "\tembeddings_matrix[i + 1] = vocab_list[i][1]\n",
    "maxlen = 300 # maxlenå¯è°æ´\n",
    "max_features = len(model_doc2vec.wv.vocab.items()) + 1\n",
    "embedding_dims = model_doc2vec.vector_size\n",
    "print(max_features, embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-15 07:44:59,205 : INFO : 100 has converted...\n",
      "2020-01-15 07:44:59,268 : INFO : 200 has converted...\n",
      "2020-01-15 07:44:59,461 : INFO : 300 has converted...\n",
      "2020-01-15 07:44:59,536 : INFO : 400 has converted...\n",
      "2020-01-15 07:44:59,609 : INFO : 500 has converted...\n",
      "2020-01-15 07:44:59,685 : INFO : 600 has converted...\n",
      "2020-01-15 07:44:59,760 : INFO : 700 has converted...\n",
      "2020-01-15 07:44:59,830 : INFO : 800 has converted...\n",
      "2020-01-15 07:44:59,910 : INFO : 900 has converted...\n",
      "2020-01-15 07:44:59,974 : INFO : 1000 has converted...\n",
      "2020-01-15 07:45:00,047 : INFO : 1100 has converted...\n",
      "2020-01-15 07:45:00,123 : INFO : 1200 has converted...\n",
      "2020-01-15 07:45:00,218 : INFO : 1300 has converted...\n",
      "2020-01-15 07:45:00,285 : INFO : 1400 has converted...\n",
      "2020-01-15 07:45:00,346 : INFO : 1500 has converted...\n",
      "2020-01-15 07:45:00,475 : INFO : 1600 has converted...\n",
      "2020-01-15 07:45:00,547 : INFO : 1700 has converted...\n",
      "2020-01-15 07:45:00,635 : INFO : 1800 has converted...\n",
      "2020-01-15 07:45:00,721 : INFO : 1900 has converted...\n",
      "2020-01-15 07:45:00,815 : INFO : 2000 has converted...\n",
      "2020-01-15 07:45:00,891 : INFO : 2100 has converted...\n",
      "2020-01-15 07:45:00,960 : INFO : 2200 has converted...\n",
      "2020-01-15 07:45:01,036 : INFO : 2300 has converted...\n",
      "2020-01-15 07:45:01,125 : INFO : 2400 has converted...\n",
      "2020-01-15 07:45:01,198 : INFO : 2500 has converted...\n",
      "2020-01-15 07:45:01,277 : INFO : 2600 has converted...\n",
      "2020-01-15 07:45:01,358 : INFO : 2700 has converted...\n",
      "2020-01-15 07:45:01,452 : INFO : 2800 has converted...\n",
      "2020-01-15 07:45:01,531 : INFO : 2900 has converted...\n",
      "2020-01-15 07:45:01,653 : INFO : 3000 has converted...\n",
      "2020-01-15 07:45:01,726 : INFO : 3100 has converted...\n",
      "2020-01-15 07:45:01,794 : INFO : 3200 has converted...\n",
      "2020-01-15 07:45:01,860 : INFO : 3300 has converted...\n",
      "2020-01-15 07:45:01,937 : INFO : 3400 has converted...\n",
      "2020-01-15 07:45:02,007 : INFO : 3500 has converted...\n",
      "2020-01-15 07:45:02,068 : INFO : 3600 has converted...\n",
      "2020-01-15 07:45:02,131 : INFO : 3700 has converted...\n",
      "2020-01-15 07:45:02,201 : INFO : 3800 has converted...\n",
      "2020-01-15 07:45:02,275 : INFO : 3900 has converted...\n",
      "2020-01-15 07:45:02,378 : INFO : 4000 has converted...\n",
      "2020-01-15 07:45:02,449 : INFO : 4100 has converted...\n",
      "2020-01-15 07:45:02,562 : INFO : 4200 has converted...\n",
      "2020-01-15 07:45:02,646 : INFO : 4300 has converted...\n",
      "2020-01-15 07:45:02,718 : INFO : 4400 has converted...\n",
      "2020-01-15 07:45:02,787 : INFO : 4500 has converted...\n",
      "2020-01-15 07:45:02,876 : INFO : 4600 has converted...\n",
      "2020-01-15 07:45:02,953 : INFO : 4700 has converted...\n",
      "2020-01-15 07:45:03,066 : INFO : 4800 has converted...\n",
      "2020-01-15 07:45:03,131 : INFO : 4900 has converted...\n",
      "2020-01-15 07:45:03,198 : INFO : 5000 has converted...\n",
      "2020-01-15 07:45:03,269 : INFO : 5100 has converted...\n",
      "2020-01-15 07:45:03,353 : INFO : 5200 has converted...\n",
      "2020-01-15 07:45:03,426 : INFO : 5300 has converted...\n",
      "2020-01-15 07:45:03,486 : INFO : 5400 has converted...\n",
      "2020-01-15 07:45:03,555 : INFO : 5500 has converted...\n",
      "2020-01-15 07:45:03,622 : INFO : 5600 has converted...\n",
      "2020-01-15 07:45:03,678 : INFO : 5700 has converted...\n",
      "2020-01-15 07:45:03,746 : INFO : 5800 has converted...\n",
      "2020-01-15 07:45:03,842 : INFO : 5900 has converted...\n",
      "2020-01-15 07:45:03,920 : INFO : 6000 has converted...\n",
      "2020-01-15 07:45:03,999 : INFO : 6100 has converted...\n",
      "2020-01-15 07:45:04,072 : INFO : 6200 has converted...\n",
      "2020-01-15 07:45:04,151 : INFO : 6300 has converted...\n",
      "2020-01-15 07:45:04,227 : INFO : 6400 has converted...\n",
      "2020-01-15 07:45:04,317 : INFO : 6500 has converted...\n",
      "2020-01-15 07:45:04,412 : INFO : 6600 has converted...\n",
      "2020-01-15 07:45:04,479 : INFO : 6700 has converted...\n",
      "2020-01-15 07:45:04,545 : INFO : 6800 has converted...\n",
      "2020-01-15 07:45:04,625 : INFO : 6900 has converted...\n",
      "2020-01-15 07:45:04,707 : INFO : 7000 has converted...\n",
      "2020-01-15 07:45:04,780 : INFO : 7100 has converted...\n",
      "2020-01-15 07:45:04,878 : INFO : 7200 has converted...\n",
      "2020-01-15 07:45:04,952 : INFO : 7300 has converted...\n",
      "2020-01-15 07:45:05,012 : INFO : 7400 has converted...\n",
      "2020-01-15 07:45:05,088 : INFO : 7500 has converted...\n",
      "2020-01-15 07:45:05,194 : INFO : 7600 has converted...\n",
      "2020-01-15 07:45:05,267 : INFO : 7700 has converted...\n",
      "2020-01-15 07:45:05,333 : INFO : 7800 has converted...\n",
      "2020-01-15 07:45:05,420 : INFO : 7900 has converted...\n",
      "2020-01-15 07:45:05,491 : INFO : 8000 has converted...\n",
      "2020-01-15 07:45:05,567 : INFO : 8100 has converted...\n",
      "2020-01-15 07:45:05,659 : INFO : 8200 has converted...\n",
      "2020-01-15 07:45:05,739 : INFO : 8300 has converted...\n",
      "2020-01-15 07:45:05,826 : INFO : 8400 has converted...\n",
      "2020-01-15 07:45:05,896 : INFO : 8500 has converted...\n",
      "2020-01-15 07:45:05,985 : INFO : 8600 has converted...\n",
      "2020-01-15 07:45:06,079 : INFO : 8700 has converted...\n",
      "2020-01-15 07:45:06,179 : INFO : 8800 has converted...\n",
      "2020-01-15 07:45:06,258 : INFO : 8900 has converted...\n",
      "2020-01-15 07:45:06,332 : INFO : 9000 has converted...\n",
      "2020-01-15 07:45:06,410 : INFO : 9100 has converted...\n",
      "2020-01-15 07:45:06,492 : INFO : 9200 has converted...\n",
      "2020-01-15 07:45:06,568 : INFO : 9300 has converted...\n",
      "2020-01-15 07:45:06,656 : INFO : 9400 has converted...\n",
      "2020-01-15 07:45:06,724 : INFO : 9500 has converted...\n",
      "2020-01-15 07:45:06,789 : INFO : 9600 has converted...\n",
      "2020-01-15 07:45:06,877 : INFO : 9700 has converted...\n",
      "2020-01-15 07:45:06,942 : INFO : 9800 has converted...\n",
      "2020-01-15 07:45:07,041 : INFO : 9900 has converted...\n",
      "2020-01-15 07:45:07,130 : INFO : 10000 has converted...\n",
      "2020-01-15 07:45:07,213 : INFO : 10100 has converted...\n",
      "2020-01-15 07:45:07,295 : INFO : 10200 has converted...\n",
      "2020-01-15 07:45:07,354 : INFO : 10300 has converted...\n",
      "2020-01-15 07:45:07,421 : INFO : 10400 has converted...\n",
      "2020-01-15 07:45:07,500 : INFO : 10500 has converted...\n",
      "2020-01-15 07:45:07,575 : INFO : 10600 has converted...\n",
      "2020-01-15 07:45:07,640 : INFO : 10700 has converted...\n",
      "2020-01-15 07:45:07,707 : INFO : 10800 has converted...\n"
     ]
    }
   ],
   "source": [
    "# è¯»å¥é¢å¤çåçææ¬ï¼æ ¹æ®é¢è®­ç»çè¯åéå­å¸æå»ºå¥å­åºå\n",
    "output_dir2 = './vectors/'\n",
    "count = 0\n",
    "for file in os.listdir(output_dir):\n",
    "    count += 1\n",
    "    with open(os.path.join(output_dir, file), 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            this_x = []\n",
    "            words = line.split(' ')\n",
    "            for word in words:\n",
    "                if word in word2idx.keys():\n",
    "                    this_x.append(word2idx[word])\n",
    "        npy_name = file[:-3]\n",
    "        npy_name = output_dir2 + npy_name + 'npy'\n",
    "        # ä¿å­\n",
    "        np.save(npy_name, this_x)   # ä¿å­ä¸º.npyæ ¼å¼\n",
    "        if count % 100 == 0:\n",
    "            logging.info('{} has converted...'.format(count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
