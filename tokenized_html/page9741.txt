pomegranate author jacob schreiber contact jmschr cs.washington.edu gerolamo cardano is one of the first europeans to publish works in the field of probability theory as a polymath he was widely versed in many areas and expressed his knowledge through a series of works at the time gerolamo was using probability mostly as an edge in gambling in order to pay off the debts he wracked up by being an academic he should have rebranded as a data scientist and collecting his 6 figure salary instead he chose to publish many seminal works including liber de ludo aleae which had an entire chapter devoted to how to cheat in gambling however the field has progressed slightly outside of the field of gambling and probabilistic modelling is now widely used in many fields one of the reasons is because like graduate students these models carry their uncertainty around with them allowing them to prroduce more robust estimates and make more informed decisions probabilistic modelling is a natural way of thinking cam davidson pilon gives an example in his excellent book bayesian methods for hackers you are a skilled programmer but bugs still slip into your code after a particularly difficult implementation of an algorithm you decide to test your code on a trivial example it passes you test the code on a harder problem it passes once again and it passes the next even more difficult test too you are starting to believe that there may be no bugs in this code this is a bayesian way of thinking as you acquire more evidence that there are no bugs in the code your belief in the correctness of the code increases but you 're never completely positive however the presence of a single bug would tell you that the code can not be bug free pomegranate initially started out as yet another hidden markov model yahmm a library written by my friend adam novak during his rotation in the ucsc nanopore lab he was disappointed in the lack of an easy installable hidden markov model library for python and so being the badass he was wrote his own from scratch in order to pursue his rotation project i then adopted his project for my own work in the nanopore lab but found that a pure python implementation of hmms was not fast enough for the massive models i was running on gigabytes of data i ended up rewriting it in cython and significantly expanding its feature set a few months after entering graduate school i realized many of the components of yahmm could be reorganized to help me finish homeworks quicker since then it has been expanded to include a wide variety of distributions finite state machines hidden markov models general mixture models bayesian networks and factor graphs more to come however while adam and i disagreed on many things such as his belief that we needed to do research as a graduate student instead of writing open source software we agreed fundamentally that the code we wrote needed to be fundamentally easy to use much like communication skills in general if what you 've done is uninterpretable to other people it will never be used installation is always as easy as pip install pomegranate note unless you 're on a windows in which case you need a 64 bit c compiler first lets go through probabilistic models of increasing complexity and how to use them in pomegranate the distribution is the simplest unit of probabilistic modelling you are saying that all of your data are realizations of the same stationary underlying distribution this is a good fit for many types of simple data where each instance is a fixed feature size and independant of all other instances here is a full list of the distributions currently supported a very common distribution is the normal distribution which is parameterized by μ \mu and σ \sigma you can create it in the following way if we want to visualize this distribution we can call the 'plot command and pass in the number of samples we want to draw this is required only because more complex distributions such as kernel densities can be difficult to plot the exact densities of good this looks like a normal distribution the most common task is to try to determine the probability of some points given the distribution p d m p d|m where the model is just this distribution the log probability for both are the same which makes sense since they 're both the same distance from 0 calculating log probabilities are more efficient than calculating probabilities and usually lead to more efficient downstream algorithms and so are the only option provided the next most common operation is to try to train this model on some data if we train a distribution we ignore its current parameterization and instead calculate the mle estimates as to the parameters lets train the distribution on points drawn from n o r m 0.4 0.9 norm 0.4 0.9 and see if we can recover the underlying distribution since we re trying to stick to scikit-learns interface as closely as possible we use the fit method we can see that we recovered the distribution fairly well pomegranate does distribution fitting in summary statistics and cython so it is significantly faster than other options lets see how long it takes to train a pomegranate distribution versus using numpy looks like it 's ~3x faster since numpy uses summary statistics these updates can be done out of core using summarize followed by from_summaries and still get exact answers summarize will summarize the data into summary statistics and allow you to get more data before finally updating the parameters of the distribution in the from_summaries method lets look at this in action expectedly we get close but not exact recovery of the parameters we can also serialize this into a json for human readability or future storage after a computationally intensive training step a second commonly used distribution is the discrete distribution which contains a fixed set of keys and their associated probabilities in pomegranate the keys can be ~any~ python object they are instantiated by passing in a dictionary with the associated keys and values all of the previous functions still work except plot we can calculate the log probability of points under this distribution which is just a dictionary lookup we can also train on lists of data and we can also serialize it into a json the above distributions were parametric in that they were defined by a handful of parameters sometimes this can lose the excentricities of the data in contrast nonparametric distributions require only data not parameters we can see this clearly below where we have some points drawn from the unit norm distribution and a few outliers we can see that the gaussian kernel densities can be especially useful in modelling outliers without having to come up with complicated distributions a downside is that calculating log probabilities takes time proportional to the number of points in the kernel density as opposed to constant time it s much like knn in that it takes o 1 o 1 to train and o n o n to predict even with intermediate state caching the gaussian kernel density is ~5x slower on only 12 points this is even after a recent update where i made them 30x faster than they used to be however it is true that kernel densities do have a parameter which is the bandwidth for gaussian kernel densities it is similar to adjusting the variance in the gaussian distribution lets take a look we can clearly see now that the outliers are dragging the normal distribution to the right but the kernel density remains unchanged we can look at a slightly more extreme example to see this truly this kernel density can thus capture heavy tailed distributions without having an unnecessarily skewed variance lastly you can also assign weights to all of the points if you choose to we can see what this looks like when we highly weight the outliers sometimes a univariate distribution is n't good enough many times data comes in the form of multiple independent dimensions each with their own distribution we can model this with a multivariate distribution which contains a tuple of independent components a common example may be to look at data whose signal is normally distributed and whose duration is modelled by an exponential we can also weight the contributions of each distribution in this example lets say that the signal is far more important than the duration we see that the point becomes more probable because we care more about the good fit to the signal dimension the normal distribution than it poorly fits the duration exponential distribution you must be careful with the weightings though because they do n't have to sum to 1 in this sense weights 3,1 is not the same as weights 0.75 0.25 we can do updates in the same manner drawing sampels from a gaussian with mu 12 and sigma 2 and an exponential with mu 5 0.2 since the inverse mu is more prominently used to parameterize an exponential we can estimate the underlying distributions with 1000 points it 's not always good enough to have independent distributions as signals can be heavily correlated a common distribution for this is the multivariate gaussian distribution utilizing a full covariance matrix you provide a vector of mus and a covariance matrix and you 're good to go we can now train on data which has a diagonal covariance matrix but whose mu 's are now 0 8 16 24 32 instead of 0 1 2 3 4 lets see how well the model captures the underlying distribution pomegranate is also faster than numpy in calculating these due to an efficient cython implementation for calculating the covariance directly instead of doing vector-matrix-vector multiplication which allocates intermediate arrays given these tools it 's pretty simple to set up a naive bayes classifier using any combination of distributions you 'd like while not currently built in you can set up the code in the following way looks like pomegranate is faster at naive bayes too but lets make sure we 've come to the same distribution in the end it is frequently the case that the data you have is not explained by a single underlying distribution if we want to try to recover the underlying distributions we need to have a model which has multiple components an example is the following data which is clearly two gaussian distributions but with different parameters we must start with an initial estimate of what this distribution is in this case we would likely initialize our general mixture model to be of two gaussian distributions one centered around 2 with a unit variance and the other centered around 8 with a unit variance it looks like there are more points in the distribution to the right so maybe we want to start off with a higher weight on that distribution before we do any training lets take a look at what the model predicts all models have a fit and a predict method in keeping with the scikit-learn api in this case we want to predict each point and see which component it falls under we started off with 2000 1 labels and 1000 0 labels so this is somewhat close it 's a decent first look but we can go on to visualize this to get a better look this looks okay we see there is a clear linear boundary between the two this makes sense because if you have two gaussians of any dimensionality the best separator is just a hyperplane this is one of the reasons that naive bayes and logistic regresson can work very well on gaussian distributed data however we are using probabilistic models here we should take advantage of this and predict the probability that each point belongs to each distribution in order to get softer estimates in keeping with the scikit-learn api if we want probabilistic measurements we use the predict_proba method and columns become each component and rows become each sample looks like we get ~1 closer this is not terribly significant but useful when we do training which uses the expectation-maximization em algorithm i wo n't go into the details here but the gist is that we want to train this distribution in an unsupervised fashion and so we iterate between expectation calculating probabilities of each point given the data and maximization updating model parameters given these probabilities we iterate between those two steps until convergence training the models exemplifies why sometimes we want to use soft classification the difference between 981 and 944 is significant particularly in fuzzy cases where it 's not always clear what 's going on as a continued theme it allows us to bring our uncertainty along with us and make more informed decisions because of this however a more common type of mixture model is the gaussian mixture model commonly confused with the general mixture model because they share similar names however the general mixture model can be a mixture of any number of distributions of heterogeneous types lets take a look at a gaussian mixture model using multivariategaussiandistributions pomegranate is several times faster than scikit-learn in this case though scikit-learn does offer more fully-featured gmms such as dpgmm and tied covariance matrices and it is a bit easier to initialize a gmm hidden markov models hmms are the flagship of the pomegranate package in that most time is spent improving their implementation and these improvements sometimes trickle down into the other algorithms lets delve into the features which pomegranate offers hidden markov models are a form of structured prediction method which extend general mixture models to sequences of data where position in the sequence is relevant if each point in this sequence is completely independent of the other points then hmms are not the right tools and gmms or more complicated bayesian networks may be a better tool the most common examples of hmms come from bioinformatics and natural language processing since i am a bioinformatician i will predominately use examples from bioinformatics lets take the simplified example of cg island detection on a sequence of dna dna is made up of the four canonical nucleotides abbreviated a c g and 't specific organizations of these nucleotides encode enough information to build you a human being one simple region in the genome is called the 'cg island where the nucleotides c and g are enriched lets compare the predictions of a gmm with the predictions of a hmm to both understand conceptually the differences between the two and to see how easy it is to use pomegranate note the hmm and gmm predictions may be the inverse of each other because hmm states undergo a topological sort in order to properly handle silent states more later which can change from the order they were inserted into the model at this point a dense hmm with equal probabilities between each state is ~equivalent~ to a gmm however this framework gives us great flexibility to add prior knowledge through edge transition probabilities whereas a gmm does n't if we look at the predictions we see that it 's bifurcating between background and cg island very quickly in essence calling every c or g a 'cg island this is not likely to be true we know that cg islands have some as and ts in them and background sequence has cs and gs we can change the transition probabilities to account for this and prevent switching from occuring too rapidly this seems far more reasonable there is a single cg island surrounded by background sequence and something at the end if we knew that cg islands can not occur at the end of sequences we need only modify the underlying structure of the hmm in order to say that the sequence must end from the background state looks like we managed to get rid of that pesky end again the numbers may have flipped look at the indices modifying transition probabilities and using non-dense graphical structures are two major ways in which hmms account for data in a sequence not being independent and identically distributed i.i.d. in fact in most applications the graphical structure of a hmm is very sparse if we want a more probabilistic view of what 's going on we can get the probability of each symbol in the sequence being in each of the states in the model easily this is useful to get a soft estimate of classification which allows us to include confidence as well as prediction values close to 50-50 get masked when you make hard classifications but this uncertainty can be passed to future applications if you use soft assignments each row in the matrix is one symbol in the sequence and the columns correspond to the two states identified above cg island or background there is a corresponding hmm.predict_log_proba method present if you want to get the log values these are the emission probability values calculated by the forward backward algorithm and can also be retrieved by calling hmm.forward_backward seq which returns both the emission and the transition probability tables lets take a look at these tables this is the transition table which has the soft count of the number of transitions across an edge in the model given a single sequence it is a square matrix of size equal to the number of states including start and end state with number of transitions from row_id to column_id this is exemplified by the 1.0 in the first row indicating that there is one transition from background state to the end state as that 's the only way to reach the end state however the third or fourth depending on ordering row is the transitions from the start state and it only slightly favors the background state these counts are not normalized to the length of the input sequence but can easily be done so by dividing by row sums column sums or entire table sums depending on your application a possible reason not to normalize is to run several sequences through and add up their tables because normalizing in the end and extracting some domain knowledge it is extremely useful in practice for example we can see that there is an expectation of 2.8956 transitions from cg island to background and 2.4 from background to cg island this could be used to infer that there are ~2-3 edges which makes sense if you consider that the start and end of the sequence seem like they might be part of the cg island states except for the strict transition probabilities used look at the first few rows of the emission table above we 've been using the forward backward algorithm and maximum a posteriori for decoding thus far however maximum a posteriori decoding has the side effect that it is possible that it predicts impossible sequences in some edge cases an alternative is viterbi decoding which at each step takes the most likely path instead of sum of all paths to produce hard assignments we see here a case in which it does not do too well the viterbi path can be more conservative in its transitions due to the hard assignments it makes in essence if multiple possibile paths are possible at a given point it takes the most likely path even if the sum of all other paths is greater than the sum of that path in problems with a lower signal to noise ratio this can mask the signal as a side note we can use the following to get the maximum a posteriori and viterbi paths lets move on to a more complicated structure that of a profile hmm a profile hmm is used to align a sequence to a reference 'profile where the reference profile can either be a single sequence or an alignment of many sequences such as a reference genome in essence this profile has a 'match state for every position in the reference profile and 'insert state and a 'delete state the insert state allows the external sequence to have an insertion into the sequence without throwing off the entire alignment such as the following or a deletion which is the opposite the bars in the middle refer to a perfect match whereas the lack of a bar means either a deletion/insertion or a mismatch a mismatch is where two positions are aligned together but do not match this models the biological phenomena of mutation where one nucleotide can convert to another over time it is usually more likely in biological sequences that this type of mutation occurs than that the nucleotide was deleted from the sequence shifting all nucleotides over by one and then another was inserted at the exact location moving all nucleotides over again since we are using a probabilistic model we get to define these probabilities through the use of distributions if we want to model mismatches we can just set our 'match state to have an appropriate distribution with non-zero probabilities over mismatches lets now create a three nucleotide profile hmm which models the sequence 'act we will fuzz this a little bit in the match states pretending to have some prior information about what mutations occur at each position if you do n't have any information setting a uniform small value over the other values is usually okay the first and last sequence are entirely matches meaning that it thinks the most likely alignment between the profile act and act is a-a c-c and t-t which makes sense and the most likely alignment between act and acc is a-a c-c and t-c which includes a mismatch essentially it 's more likely that there 's a t-c mismatch at the end then that there was a deletion of a t at the end of the sequence and a separate insertion of a c. the two middle sequences do n't match very well as expected g 's are not very likely in this profile at all it predicts that the two g 's are inserts and that the c matches the c in the profile before hitting the delete state because it ca n't emit a t. the third sequence thinks that the g is an insert as expected and then aligns the a and t in the sequence to the a and t in the master sequence missing the middle c in the profile by using deletes we can handle other sequences which are shorter than three characters lets look at some more sequences of different lengths again more of the same expected you 'll notice most of the use of insertion states are at i0 because most of the insertions are at the beginning of the sequence it 's more probable to simply stay in i0 at the beginning instead of go from i0 to d1 to i1 or going to another insert state along there you 'll see other insert states used when insertions occur in other places in the sequence like 'attt and 'acgtg now that we have the path we need to convert it into an alignment which is significantly more informative to look at there are two main algorithms for training hidden markov models baum welch structured version of expectation maximization and viterbi training since we do n't start off with labels on the data these are both unsupervised training algorithms in order to assign labels baum welch uses em to assign soft labels weights in this case to each point belonging to each state and then using weighted mle estimates to update the distributions viterbi assigns hard labels to each observation using the viterbi algorithm and then updates the distributions based on these hard labels pomegranate is extremely well featured when it comes to regularization methods for training supporting tied emissions and edges edge and emission inertia freezing nodes or edges edge pseudocounts and multithreaded training see this tutorial for more information bayesian networks are a powerful probabilistic inference tool in which a set of variables are represented as nodes and the lack of an edge represents a conditional independence statement between them bayesian networks are powerful in their ability to infer the value of unknown variables given any combination of known variables obviously as more is known about the system the more accurate these inferences will be but the same algorithm can handle very different amounts of information while bayesian networks can have extremely complex emission probabilities usually gaussian or conditional gaussian distributions pomegranate currently supports only discrete bayesian networks in order to do inference factor graph belief propogation is used if you did n't understand that it 's okay lets look at the simple monty hall example the monty hall problem arose from the gameshow let 's make a deal where a guest had to choose which one of three doors had a car behind it the twist was that after the guest chose the host originally monty hall would then open one of the doors the guest did not pick and ask if the guest wanted to switch which door they had picked initial inspection may lead you to believe that if there are only two doors left there is a 50-50 chance of you picking the right one and so there is no advantage one way or the other however it has been proven both through simulations and analytically that there is in fact a 66 chance of getting the prize if the guest switches their door regardless of the door they initially went with we can reproduce this result using bayesian networks with three nodes one for the guest one for the prize and one for the door monty chooses to open the door the guest initially chooses and the door the prize is behind are completely random processes across the three doors but the door which monty opens is dependent on both the door the guest chooses it can not be the door the guest chooses and the door the prize is behind it can not be the door with the prize behind it pomegranate uses loopy belief propogation on the factor graph to calculate marginals meaning that it is an inexact algorithm but converges to the exact solution on bayesian networks which have a tree structure we can use the predict_proba method in order to ask questions for single data points as a baseline lets see what happens if we do n't put in any information this should give us the marginal of the graph which is that everything is equally likely now lets do something different and say that the guest has chosen door a we do this by passing a dictionary to predict_proba with key pairs consisting of the name of the state in the state object and the value which that variable has taken we can see that now monty will not open door a because the guest has chosen it at the same time the distribution over the prize has not changed it is still equally likely that the prize is behind each door now lets say that monty opens door c and see what happens