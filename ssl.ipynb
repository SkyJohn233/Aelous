{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Semi-supervised learning on renders \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install \"tensorflow_hub>=0.6.0\"\n",
    "# !pip install \"tensorflow>=2.0.0\" uncomment this if tensorflow not found\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Activation, Lambda\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, Reshape\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Image\n",
    "    \n",
    "IMG_WIDTH, IMG_HEIGHT = 224,224\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_png(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  img_resize = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "  return img_resize\n",
    "\n",
    "def process_path(file_path):\n",
    "  # label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, img\n",
    "  # return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'../render/render2288.png', shape=(), dtype=string)\n",
      "tf.Tensor(b'../render/render344.png', shape=(), dtype=string)\n",
      "tf.Tensor(b'../render/render1306.png', shape=(), dtype=string)\n",
      "tf.Tensor(b'../render/render9642.png', shape=(), dtype=string)\n",
      "tf.Tensor(b'../render/render5508.png', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str('../render/render*'))\n",
    "for f in list_ds.take(5):\n",
    "  print(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-99ccf7c67bf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Count positive samples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 256\n",
    "image_count = 10394\n",
    "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "for item, mirror_item in labeled_ds.take(1):\n",
    "    ImgShape = item.shape\n",
    "# df = pd.DataFrame(labeled_ds)\n",
    "# df.info()\n",
    "\n",
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "  # This is a small dataset, only load it once, and keep it in memory.\n",
    "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "  # fit in memory.\n",
    "  if cache:\n",
    "    if isinstance(cache, str):\n",
    "      ds = ds.cache(cache)\n",
    "    else:\n",
    "      ds = ds.cache()\n",
    "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "  # Repeat forever\n",
    "  ds = ds.repeat()\n",
    "\n",
    "  ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "  # is training.\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "  return ds\n",
    "\n",
    "class CollectBatchStats(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    self.batch_losses = []\n",
    "    self.batch_acc = []\n",
    "\n",
    "  def on_train_batch_end(self, batch, logs=None):\n",
    "    self.batch_losses.append(logs['loss'])\n",
    "    self.batch_acc.append(logs['acc'])\n",
    "    self.model.reset_metrics()\n",
    "    \n",
    "\n",
    "num_class = 10    \n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "def classifier_func(x):\n",
    "    return x+x*K.one_hot(K.argmax(x, axis=1), num_classes=num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetModel(input_shape):\n",
    "    input_img = Input(shape=(224, 224, 3))\n",
    "\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = MaxPooling2D((2, 2), padding='same', strides=(2, 2))(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same', strides=(2, 2))(x)\n",
    "    x = Conv2D(4, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same', strides=(2, 2))(x)\n",
    "    x = Conv2D(2, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same', strides=(2, 2))(x)\n",
    "    \n",
    "    # Output Shape: 4x4x8\n",
    "    x = Conv2D(2, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(4, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    # Output Shape: 28x28x1\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer=\"adadelta\", loss=\"mse\", metrics=['acc']) # may add f1_score later\n",
    "    autoencoder.summary()\n",
    "    return autoencoder\n",
    "\n",
    "def GetModel2(input_shape):\n",
    "    input_img = Input(shape=(224, 224, 3))\n",
    "    #Encoder:\n",
    "    conv_1 = Conv2D(16, (3,3), strides=(1,1))(input_img)\n",
    "    act_1 = Activation('relu')(conv_1)\n",
    "    maxpool_1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(act_1)\n",
    "\n",
    "    conv_2 = Conv2D(4, (3,3), strides=(1,1), padding='same')(maxpool_1)\n",
    "    act_2 = Activation('relu')(conv_2)\n",
    "    maxpool_2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(act_2)\n",
    "    # Output Shape: 6x6x64\n",
    "    conv_3 = Conv2D(1, (3,3), strides=(1,1), padding='same')(maxpool_2)\n",
    "    act_3 = Activation('relu')(conv_3)\n",
    "    maxpool_3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(act_3)\n",
    "\n",
    "    flat_1 = Flatten()(maxpool_3)\n",
    "\n",
    "    fc_1 = Dense(256)(flat_1)\n",
    "    act_3 = Activation('relu')(fc_1)\n",
    "\n",
    "    fc_2 = Dense(128)(act_3)\n",
    "    act_4 = Activation('relu')(fc_2)\n",
    "\n",
    "    fc_3 = Dense(num_class)(act_4)\n",
    "\n",
    "    act_class = Lambda(classifier_func, output_shape=(num_class,))(fc_3)\n",
    "    # Output Shape: 10\n",
    "\n",
    "    #Decoder:\n",
    "    fc_4 = Dense(256)(act_class)\n",
    "    act_5 = Activation('relu')(fc_4)\n",
    "\n",
    "    fc_5 = Dense(2304)(act_5)\n",
    "    act_6 = Activation('relu')(fc_5)\n",
    "    reshape_1 = Reshape((6,6,16))(act_6)\n",
    "    \n",
    "    \n",
    "    upsample_1 = UpSampling2D((2, 2))(reshape_1)\n",
    "    deconv_1 = Conv2DTranspose(16, (3, 3), strides=(1, 1))(upsample_1)\n",
    "    act_7 = Activation('relu')(deconv_1)\n",
    "\n",
    "    upsample_2 = UpSampling2D((2, 2))(act_7)\n",
    "    deconv_2 = Conv2DTranspose(8, (3, 3), strides=(1, 1))(upsample_2)\n",
    "    act_8 = Activation('relu')(deconv_2)\n",
    "    \n",
    "    upsample_3 = UpSampling2D((2, 2))(act_8)\n",
    "    deconv_3 = Conv2DTranspose(1, (3, 3), strides=(1, 1))(upsample_3)\n",
    "    act_9 = Activation('relu')(deconv_3)\n",
    "\n",
    "    conv_3 = Conv2D(3, (3, 3), strides=(1, 1))(act_9)\n",
    "    act_10 = Activation('sigmoid')(conv_3)\n",
    "    # Output Shape: 28x28x1\n",
    "\n",
    "    autoencoder = Model(input_img, act_10)\n",
    "    autoencoder.summary()\n",
    "    return autoencoder\n",
    "\n",
    "Dataset_train = prepare_for_training(labeled_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 224, 224, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 112, 112, 8)       1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 56, 56, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 56, 56, 4)         292       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 28, 28, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 28, 28, 2)         74        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 14, 14, 2)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 14, 14, 2)         38        \n",
      "_________________________________________________________________\n",
      "up_sampling2d_35 (UpSampling (None, 28, 28, 2)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 28, 28, 4)         76        \n",
      "_________________________________________________________________\n",
      "up_sampling2d_36 (UpSampling (None, 56, 56, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 56, 56, 8)         296       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_37 (UpSampling (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 112, 112, 16)      1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_38 (UpSampling (None, 224, 224, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 224, 224, 3)       435       \n",
      "=================================================================\n",
      "Total params: 3,987\n",
      "Trainable params: 3,987\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 224, 224, 3), (None, 224, 224, 3)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = GetModel(input_shape=ImgShape)\n",
    "Dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 41.0 steps\n",
      "Epoch 1/3\n",
      "41/41 [==============================] - 287s 7s/step - loss: 0.1867 - acc: 0.7931\n",
      "Epoch 2/3\n",
      "41/41 [==============================] - 263s 6s/step - loss: 0.1865 - acc: 0.7969\n",
      "Epoch 3/3\n",
      "41/41 [==============================] - 262s 6s/step - loss: 0.1866 - acc: 0.7872\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = './saved_model/autoencoder.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-fd04f0236c1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                               callbacks = [batch_stats_callback])\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./saved_model/autoencoder.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \"\"\"\n\u001b[1;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1008\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    110\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 112\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = './saved_model/autoencoder.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use unlabeled data to train the Encoder\n",
    "# autoencoder.fit(X_train, X_train, steps_per_epoch=16, epochs=10, verbose=1,\n",
    "#                shuffle=True)\n",
    "\n",
    "# Data_train = tf.data.Dataset.from_tensor_slices((X_train, X_train))\n",
    "\n",
    "batch_stats_callback = CollectBatchStats()\n",
    "history = autoencoder.fit_generator(Dataset_train, epochs=3,\n",
    "                              steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                              callbacks = [batch_stats_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('./saved_model/autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = tf.keras.models.load_model('./saved_model/autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-4aac0f3f7644>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdata_labeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/dataset_labeled.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mX_labeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m799\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mX_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_representation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labeled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_representation = Sequential()\n",
    "hidden_representation.add(autoencoder.layers[0])\n",
    "hidden_representation.add(autoencoder.layers[1])\n",
    "hidden_representation.add(autoencoder.layers[2])\n",
    "hidden_representation.add(autoencoder.layers[3])\n",
    "hidden_representation.add(autoencoder.layers[4])\n",
    "hidden_representation.add(autoencoder.layers[5])\n",
    "hidden_representation.add(autoencoder.layers[6])\n",
    "hidden_representation.add(autoencoder.layers[7])\n",
    "hidden_representation.add(autoencoder.layers[8])\n",
    "\n",
    "\n",
    "data_labeled = pd.read_csv('./data/dataset_labeled.csv')\n",
    "X_labeled = X_train[0:799]\n",
    "\n",
    "X_rep = hidden_representation.predict(X_labeled)\n",
    "\n",
    "# X_labeled = vectorizer.fit_transform(X_labeled)\n",
    "y_rep = data_labeled['NewCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
