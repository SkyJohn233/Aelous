How to troubleshoot pBlk exhaustion due to Slow Authentication bars netapp-mark NetApp chevron-down comment globe Find a NetApp office near you.               See our worldwide contact list While not as common as Fpolicy-related or Vscan-related pBlk  exhaustion, there have been instances where the authentication of users  against a Storage Controller has resulted in pBlk exhaustion. This  scenario tends to happen more frequently when many users log in at once  (for example, Monday morning at 8 am). In a situation where 'slow  authentication' could be contributing to pBlk exhaustion, the output of  stats show  < CIFS_related_objects > ,  cifs stat , and the  /etc/message  log will be used for diagnosis. When troubleshooting pBlk exhaustion that could be the result of slow  authentication, one or more of the following systems can be involved: A high-level overview of the authentication process is as follows: The UNIX usermapping process (step #5) is a step that occurs every  time a Windows client connects to a NetApp Storage Controller and it is  performed during the authentication process. Regardless of the  security-style of a volume or qtree that the client intends to access,  all Windows users are mapped to UNIX users. A high-level overview of the UNIX usermapping process is as follows: The following is required to troubleshoot pBlk exhaustion due to slow authentication: FILER_CLI:  >  options autosupport.doit pblktroubleshooting FILER_CLI: >  priv set diag; cifs stat -z Gather the following statistics to help determine the source of the slow authentication. This will gather 60 seconds of statistics around how long a domain controller is taking to process the authentication requests. It will also provide which type of authentication clients are requesting: NTLM or Kerberos. FILER_CLI* >  stats show -i 1 -n 60 cifsdomain  The output of the command will look similar to the following: FILER_CLI* >  stats show -i 1 -n 60 cifsdomain Instance  netlogon_lat netlogon_lat  lsa_latency  lsa_latency_  samr_latency  samr_latency 192.168.1.100  0  0  0  0  0  0 192.168.1.150  0  0  0  0  0  0 192.168.1.200  0  0  0  0  0  0 Each value has two entries, one for the actual latency and one for the base used to calculate the latency. Since the command is using one second iterations, the latency base represents how many requests the Storage Controller serviced per second. In addition, each call represents the following: Netlogon Latency: This counter represents every time the Storage Controller contacted a Domain Controller to complete NTLM Authentication.  LSA Latency: This counter represents every time the Storage Controller contacted a Domain Controller to resolve a user SID to a Name (Sid2Name.) There are two events that trigger a Sid2Name lookup: SAMR Latency: This counter represents every time the Storage Controller contacted a Domain Controller to look up Windows Group membership. This event is triggered whenever UNIX User connects to a Mixed or NTFS Qtree requiring the UNIX- to-Windows Usermapping, which results in Windows Group lookups of the UNIX User. Once the data has been collected, compare the output to the scenarios outlined below, and then select the one with the closest match. Note:  Without the fix for BUG  552466 , the output from the  cifsdomain stat  only reflects the authentication activity for vfiler0. Sc enario #1 : The output of the stats collected reports the counters reflect high latency in the netlogon counter columns. FILER_CLI* >  stats show -i 1 -n 60 cifsdomain Instance  netlogon_lat netlogon_lat  lsa_latency  lsa_latency_  samr_latency  samr_latency 192.168.1.100  100  100  10  50  0  0 192.168.1.150  100   50  5   b  25  0  0 In this scenario, the domain controllers are handling approximately 50-100 authentication requests per second (second column - base). Those requests are taking on average 100ms to complete (first column - latency) to complete. This indicates the clients are primarily using NTLM for authentication. Resolution: If the SPN is not defined correctly for the Storage Controller’s machine  account in Active Directory, all Windows Clients will fall back to  NTLM. This most commonly happens when  options dns.domainname  does not  match the FQDN of the domain that the Storage Controller is joined to, or you are accessing the storage controller via NetBIOS alias or CNAME. By  default, Data ONTAP will add four SPN’s,  HOST/Filer ,   HOST/Filer.domain.com ,  NFS/Filer , and  NFS/Filer.domain.com . If there are no improvements after checking and making any appropriate adjustments, then  further investigation into network and/or Domain Controller performance should be done. Scenario #2 : The output of the stats collected reports the counters reflect high latency in the lsa counter columns. FILER_CLI* >  stats show -i 1 -n 60 cifsdomain Instance  netlogon_lat netlogon_lat  lsa_latency  lsa_latency_  samr_latency  samr_latency 192.168.1.100  10   10   100   100  0  0 192.168.1.150  5  5  100   50  0  0 In a normal Windows workload, the lsa latency base counter will generally be low. There are two known situations where the counters will increase and which can contribute to pBlk exhaustion: In this scenario, it indicates the clients are primarily using Kerberos  for authentication. When Kerberos is used, the storage controller might  need to conduct a SID-to-Name lookup. A client that attempts to  authenticate using Kerberos will provide the storage controller with the  users SID. If the storage controller does not have the SID-to-Name  cached, Data ONTAP will have to issue an LSARPC call to a DC to complete  this mapping. Once the information is received, Data ONTAP will cache this information for 24 hours by default. The following two Data ONTAP options  control sidcache: cifs.sidcache.enable  and  cifs.sidcache.lifetime . The same SID-to-Name mapping occurs when a UNIX user mounts an NFS export where the security style of the volume or qtree is NTFS. Therefore, these counters should be expected to be higher in a Multiprotocol environment. Resolution: If there are no improvments after checking and making any appropriate  adjustments, then  further investigation into network and/or Domain  Controller performance should be carried out. Scenario #3 : There are no known issues where a high reported value for SAMR_latency can be addressed from a NetApp perspective. Performance of the domain controllers or the network should be evaluated if the SAMR_latency values appear to be abnormally high. Note : If you have reviewed the three scenarios above and are still having an  issue, you will need to look into the usermapping process (Scenario #4  (LDAP) and Scenario #5 (NIS)). This will involve some additional data collection  depending on your environment. The usermapping process can utilize  LDAP or NIS. If you have configured both, then collect the data below  as specified, otherwise only collect the data necessary (For example, if you  do not have 'options ldap' configured, but have NIS configured, then only collect the statistics for NIS as defined below). Scenario #4 : You have options ldap' configured. You have 'options ldap' configured and ldap is specified in the passwd and group lines in the  /etc/nsswitch.conf  file: FILER_CLI: >  priv set diag; stats show -i 1 -n 60 ldap The output of the command will look similar to the following: Instance avg_latency latency_base  ldap  153   50  ldap  153   51  ldap  153   50 The example above shows the storage controller receiving 50 reqs/sec averaging 153ms per request. This computes to a queue time of 7.6 seconds of work incoming every second (avg_latency * latency_base = N, for example: .153 * 50 = 7.65 ). Resolution: If the latency of LDAP is high (50 ms or greater), it is likely  that it is a contributor to pBlk exhaustion. As an immediate test, you can disable UNIX ldap, this will clear the issue instantly: FILER_CLI: >  options ldap.enable off If the issue subsides, then you can work to further narrow down what aspect  of LDAP is contributing to the pBlk exhaustion, by completing the following steps: options ldap.enable on FILER_CLI: >  pktt start all -d /etc/crash Start  >  Run  >  \StorageController FILER_CLI: >  pktt stop all Once a packet trace has been collected, it will need to be analyzed  with  Wireshark  . After the trace is opened, apply the following filter: smb.cmd == 0x73 or tcp.port == 389 Note : If you are using a port other than 389, change the port number to that value. After the trace is opened and filtered, look for a Session Setup  AndX Request and note the time difference between the Session Setup AndX Request and the  corresponding Session Setup AndX Response. If it is more than a few milliseconds,  examine the LDAP calls that are between the Session Setup AndX Request  and Session Setup AndX Response. If the LDAP calls have a response time  greater than 50 milliseconds, it is likely to be the cause and will need to  be examined to increase the performance. If the performance cannot be  improved, perform the following steps: Disable secondary group lookups as most customers do not use  them. Only disable secondary groups after asking the UNIX admin to  verify that they do not use them. To disable this feature, modify the  /etc/nsswitch.conf  to remove the ldap entry from the group line: From:  group: files nis ldap To:  group: files nis If after disabling Secondary Groups, pBlk exhaustion continues  to occur, explore disabling LDAP completely until the performance of the LDAP  server can be improved: FILER_CLI: >  options ldap.enable off If you cannot turn off ldap completely, you can also remove the ldap entry from the passwd line and the group line in the  /etc/nsswitch.conf  file until you can resolve the LDAP server performance. From:  passwd: files nis ldap group: file nis ldap To:  passwd: files nis group: files nis Scenario #5 : In addition to LDAP, you may also be using NIS for user and group lookups. While there is the potential for NIS server lookups to cause pBlk exhaustion, those instances are now rare once the NIS slave feature was added to Data ONTAP.  To troubleshoot NIS, perform the following steps: FILER_CLI: >  nis info  NIS domain is  nis.domain.com  NIS group cache has been enabled  The group cache is not available.  IP Address Type State Bound Last Polled Client calls Became Active  -------------------------------------------------------------------  a.b.c.d  PREF NO RESP NO Sun Feb 8 19:03:10 GMT 2009 0  NIS Performance Statistics:  Number of YP Lookups: 4340  Total time spent in YP Lookups: 35568 ms, 746 us  Number of network re-transmissions: 0  Minimum time spent in a YP Lookup: 0 ms, 0 us  Maximum time spent in a YP Lookup: 985 ms, 903 us  Average time spent in YP Lookups: 8 ms, 195 us  3 Most Recent Lookups:  [0] Lookup time: 5 ms, 984 us Number of network re-transmissions: 0  [1] Lookup time: 5 ms, 72 us Number of network re-transmissions: 0  [2] Lookup time: 9 ms, 3 us Number of network re-transmissions: 0  NIS netgroup (*.* and *.nisdomain) cache status:  Netgroup cache: uninitialized  *.* eCode: 0  *.nisdomain eCode: 0  NIS Slave disabled Resolution: If NIS is determined to be contributing to pBlk exhaustion you can disable NIS: Perform the following steps: FILER_CLI: >  options nis.slave.enable on Note : If after verifying everything above (including troubleshooting NIS and LDAP),  if you still need assistance in troubleshooting pBlk exhaustion, open a support case with any data you collected as part of troubleshooting the issue through this article. Related Links : For more information on user authentication, see the following articles:  This web content has been translated for your convenience using a machine translation software powered by SDL. Reasonable efforts have been made to provide an accurate translation, however, no automated translation is perfect nor is it intended to replace human translators. blog community twitter facebook linkedin youtube slideshare Have feedback for our website?           Let us know Action Capture 