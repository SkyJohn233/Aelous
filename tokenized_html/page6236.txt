how to troubleshoot pblk exhaustion due to slow authentication bars netapp-mark netapp chevron-down comment globe find a netapp office near you see our worldwide contact list while not as common as fpolicy-related or vscan-related pblk exhaustion there have been instances where the authentication of users against a storage controller has resulted in pblk exhaustion this scenario tends to happen more frequently when many users log in at once for example monday morning at 8 am in a situation where 'slow authentication could be contributing to pblk exhaustion the output of stats show cifs_related_objects cifs stat and the /etc/message log will be used for diagnosis when troubleshooting pblk exhaustion that could be the result of slow authentication one or more of the following systems can be involved a high-level overview of the authentication process is as follows the unix usermapping process step 5 is a step that occurs every time a windows client connects to a netapp storage controller and it is performed during the authentication process regardless of the security-style of a volume or qtree that the client intends to access all windows users are mapped to unix users a high-level overview of the unix usermapping process is as follows the following is required to troubleshoot pblk exhaustion due to slow authentication filer_cli options autosupport.doit pblktroubleshooting filer_cli priv set diag cifs stat -z gather the following statistics to help determine the source of the slow authentication this will gather 60 seconds of statistics around how long a domain controller is taking to process the authentication requests it will also provide which type of authentication clients are requesting ntlm or kerberos filer_cli* stats show -i 1 -n 60 cifsdomain the output of the command will look similar to the following filer_cli* stats show -i 1 -n 60 cifsdomain instance netlogon_lat netlogon_lat lsa_latency lsa_latency_ samr_latency samr_latency 192.168.1.100 0 0 0 0 0 0 192.168.1.150 0 0 0 0 0 0 192.168.1.200 0 0 0 0 0 0 each value has two entries one for the actual latency and one for the base used to calculate the latency since the command is using one second iterations the latency base represents how many requests the storage controller serviced per second in addition each call represents the following netlogon latency this counter represents every time the storage controller contacted a domain controller to complete ntlm authentication lsa latency this counter represents every time the storage controller contacted a domain controller to resolve a user sid to a name sid2name there are two events that trigger a sid2name lookup samr latency this counter represents every time the storage controller contacted a domain controller to look up windows group membership this event is triggered whenever unix user connects to a mixed or ntfs qtree requiring the unix- to-windows usermapping which results in windows group lookups of the unix user once the data has been collected compare the output to the scenarios outlined below and then select the one with the closest match note without the fix for bug 552466 the output from the cifsdomain stat only reflects the authentication activity for vfiler0 sc enario 1 the output of the stats collected reports the counters reflect high latency in the netlogon counter columns filer_cli* stats show -i 1 -n 60 cifsdomain instance netlogon_lat netlogon_lat lsa_latency lsa_latency_ samr_latency samr_latency 192.168.1.100 100 100 10 50 0 0 192.168.1.150 100 50 5 b 25 0 0 in this scenario the domain controllers are handling approximately 50-100 authentication requests per second second column base those requests are taking on average 100ms to complete first column latency to complete this indicates the clients are primarily using ntlm for authentication resolution if the spn is not defined correctly for the storage controller s machine account in active directory all windows clients will fall back to ntlm this most commonly happens when options dns.domainname does not match the fqdn of the domain that the storage controller is joined to or you are accessing the storage controller via netbios alias or cname by default data ontap will add four spn s host/filer host/filer.domain.com nfs/filer and nfs/filer.domain.com if there are no improvements after checking and making any appropriate adjustments then further investigation into network and/or domain controller performance should be done scenario 2 the output of the stats collected reports the counters reflect high latency in the lsa counter columns filer_cli* stats show -i 1 -n 60 cifsdomain instance netlogon_lat netlogon_lat lsa_latency lsa_latency_ samr_latency samr_latency 192.168.1.100 10 10 100 100 0 0 192.168.1.150 5 5 100 50 0 0 in a normal windows workload the lsa latency base counter will generally be low there are two known situations where the counters will increase and which can contribute to pblk exhaustion in this scenario it indicates the clients are primarily using kerberos for authentication when kerberos is used the storage controller might need to conduct a sid-to-name lookup a client that attempts to authenticate using kerberos will provide the storage controller with the users sid if the storage controller does not have the sid-to-name cached data ontap will have to issue an lsarpc call to a dc to complete this mapping once the information is received data ontap will cache this information for 24 hours by default the following two data ontap options control sidcache cifs.sidcache.enable and cifs.sidcache.lifetime the same sid-to-name mapping occurs when a unix user mounts an nfs export where the security style of the volume or qtree is ntfs therefore these counters should be expected to be higher in a multiprotocol environment resolution if there are no improvments after checking and making any appropriate adjustments then further investigation into network and/or domain controller performance should be carried out scenario 3 there are no known issues where a high reported value for samr_latency can be addressed from a netapp perspective performance of the domain controllers or the network should be evaluated if the samr_latency values appear to be abnormally high note if you have reviewed the three scenarios above and are still having an issue you will need to look into the usermapping process scenario 4 ldap and scenario 5 nis this will involve some additional data collection depending on your environment the usermapping process can utilize ldap or nis if you have configured both then collect the data below as specified otherwise only collect the data necessary for example if you do not have 'options ldap configured but have nis configured then only collect the statistics for nis as defined below scenario 4 you have options ldap configured you have 'options ldap configured and ldap is specified in the passwd and group lines in the /etc/nsswitch.conf file filer_cli priv set diag stats show -i 1 -n 60 ldap the output of the command will look similar to the following instance avg_latency latency_base ldap 153 50 ldap 153 51 ldap 153 50 the example above shows the storage controller receiving 50 reqs/sec averaging 153ms per request this computes to a queue time of 7.6 seconds of work incoming every second avg_latency latency_base n for example .153 50 7.65 resolution if the latency of ldap is high 50 ms or greater it is likely that it is a contributor to pblk exhaustion as an immediate test you can disable unix ldap this will clear the issue instantly filer_cli options ldap.enable off if the issue subsides then you can work to further narrow down what aspect of ldap is contributing to the pblk exhaustion by completing the following steps options ldap.enable on filer_cli pktt start all -d /etc/crash start run \storagecontroller filer_cli pktt stop all once a packet trace has been collected it will need to be analyzed with wireshark after the trace is opened apply the following filter smb.cmd == 0x73 or tcp.port == 389 note if you are using a port other than 389 change the port number to that value after the trace is opened and filtered look for a session setup andx request and note the time difference between the session setup andx request and the corresponding session setup andx response if it is more than a few milliseconds examine the ldap calls that are between the session setup andx request and session setup andx response if the ldap calls have a response time greater than 50 milliseconds it is likely to be the cause and will need to be examined to increase the performance if the performance can not be improved perform the following steps disable secondary group lookups as most customers do not use them only disable secondary groups after asking the unix admin to verify that they do not use them to disable this feature modify the /etc/nsswitch.conf to remove the ldap entry from the group line from group files nis ldap to group files nis if after disabling secondary groups pblk exhaustion continues to occur explore disabling ldap completely until the performance of the ldap server can be improved filer_cli options ldap.enable off if you can not turn off ldap completely you can also remove the ldap entry from the passwd line and the group line in the /etc/nsswitch.conf file until you can resolve the ldap server performance from passwd files nis ldap group file nis ldap to passwd files nis group files nis scenario 5 in addition to ldap you may also be using nis for user and group lookups while there is the potential for nis server lookups to cause pblk exhaustion those instances are now rare once the nis slave feature was added to data ontap to troubleshoot nis perform the following steps filer_cli nis info nis domain is nis.domain.com nis group cache has been enabled the group cache is not available ip address type state bound last polled client calls became active a.b.c.d pref no resp no sun feb 8 19:03:10 gmt 2009 0 nis performance statistics number of yp lookups 4340 total time spent in yp lookups 35568 ms 746 us number of network re-transmissions 0 minimum time spent in a yp lookup 0 ms 0 us maximum time spent in a yp lookup 985 ms 903 us average time spent in yp lookups 8 ms 195 us 3 most recent lookups 0 lookup time 5 ms 984 us number of network re-transmissions 0 1 lookup time 5 ms 72 us number of network re-transmissions 0 2 lookup time 9 ms 3 us number of network re-transmissions 0 nis netgroup and *.nisdomain cache status netgroup cache uninitialized ecode 0 *.nisdomain ecode 0 nis slave disabled resolution if nis is determined to be contributing to pblk exhaustion you can disable nis perform the following steps filer_cli options nis.slave.enable on note if after verifying everything above including troubleshooting nis and ldap if you still need assistance in troubleshooting pblk exhaustion open a support case with any data you collected as part of troubleshooting the issue through this article related links for more information on user authentication see the following articles this web content has been translated for your convenience using a machine translation software powered by sdl reasonable efforts have been made to provide an accurate translation however no automated translation is perfect nor is it intended to replace human translators blog community twitter facebook linkedin youtube slideshare have feedback for our website let us know action capture