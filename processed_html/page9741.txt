pomegranate author: Jacob Schreiber  contact: jmschr@cs.washington.edu Gerolamo Cardano is one of the first Europeans to publish works in the field of probability theory. As a polymath, he was widely versed in many areas, and expressed his knowledge through a series of works. At the time, Gerolamo was using probability mostly as an edge in gambling in order to pay off the debts he wracked up by being an academic. He should have rebranded as a data scientist and collecting his 6 figure salary. Instead, he chose to publish many seminal works, including  Liber de ludo aleae , which had an entire chapter devoted to how to cheat in gambling. However, the field has progressed slightly outside of the field of gambling, and probabilistic modelling is now widely used in many fields. One of the reasons is because, like graduate students, these models carry their uncertainty around with them, allowing them to prroduce more robust estimates and make more informed decisions. Probabilistic modelling is a natural way of thinking. Cam Davidson Pilon gives an example in his excellent book  Bayesian Methods for Hackers : You are a skilled programmer, but bugs still slip into your code. After a particularly difficult implementation of an algorithm, you decide to test your code on a trivial example. It passes. You test the code on a harder problem. It passes once again. And it passes the next,  even more difficult , test too! You are starting to believe that there may be no bugs in this code... This is a Bayesian way of thinking. As you acquire more evidence that there are no bugs in the code, your belief in the correctness of the code increases, but you're never completely positive. However, the presence of a single bug would tell you that the code cannot be bug free. pomegranate initially started out as Yet Another Hidden Markov Model (yahmm), a library written by my friend Adam Novak during his rotation in the UCSC Nanopore Lab. He was disappointed in the lack of an easy installable hidden Markov model library for Python, and so, being the badass he was, wrote his own from scratch in order to pursue his rotation project. I then adopted his project for my own work in the nanopore lab, but found that a pure python implementation of HMMs was not fast enough for the massive models I was running on gigabytes of data. I ended up rewriting it in Cython, and significantly expanding its feature set. A few months after entering graduate school, I realized many of the components of yahmm could be reorganized to help me finish homeworks quicker. Since then, it has been expanded to include a wide variety of distributions, finite state machines, hidden Markov models, general mixture models, Bayesian networks, and factor graphs. More to come! However, while Adam and I disagreed on many things, such as his belief that we needed to do "research" as a graduate student instead of writing open source software, we agreed fundamentally that the code we wrote needed to be fundamentally easy to use. Much like communication skills in general, if what you've done is uninterpretable to other people, it will never be used. Installation is always as easy as  pip install pomegranate  (NOTE: Unless you're on a windows, in which case you need a 64 bit C compiler first). Lets go through probabilistic models of increasing complexity, and how to use them in pomegranate. The distribution is the simplest unit of probabilistic modelling. You are saying that all of your data are realizations of the same stationary underlying distribution. This is a good fit for many types of simple data where each instance is a fixed feature size and independant of all other instances. Here is a full list of the distributions currently supported: A very common distribution is the normal distribution which is parameterized by  μ \mu  and  σ \sigma . You can create it in the following way: If we want to visualize this distribution, we can call the 'plot' command, and pass in the number of samples we want to draw. This is required only because more complex distributions, such as kernel densities, can be difficult to plot the exact densities of. Good, this looks like a normal distribution. The most common task is to try to determine the probability of some points given the distribution,  P ( D | M ) P(D|M) , where the model is just this distribution. The log probability for both are the same, which makes sense since they're both the same distance from 0. Calculating log probabilities are more efficient than calculating probabilities, and usually lead to more efficient downstream algorithms, and so are the only option provided. The next most common operation is to try to train this model on some data. If we train a distribution, we ignore its current parameterization, and instead calculate the MLE estimates as to the parameters. Lets train the distribution on points drawn from  N o r m ( 0.4 , 0.9 ) Norm(0.4, 0.9) , and see if we can recover the underlying distribution. Since we ' re trying to stick to scikit-learns interface as closely as possible, we use the fit method. We can see that we recovered the distribution fairly well. pomegranate does distribution fitting in summary statistics and cython, so it is significantly faster than other options. Lets see how long it takes to train a pomegranate distribution, versus using numpy. Looks like it's ~3x faster. Since numpy uses summary statistics, these updates can be done out of core using  summarize  followed by  from_summaries , and still get exact answers.  summarize  will summarize the data into summary statistics, and allow you to get more data, before finally updating the parameters of the distribution in the  from_summaries  method. Lets look at this in action. Expectedly, we get close, but not exact, recovery of the parameters. We can also serialize this into a JSON for human readability, or future storage after a computationally intensive training step. A second commonly used distribution is the discrete distribution, which contains a fixed set of keys, and their associated probabilities. In pomegranate, the keys can be ~any~ Python object. They are instantiated by passing in a dictionary with the associated keys and values. All of the previous functions still work (except plot). We can calculate the log probability of points under this distribution, which is just a dictionary lookup. We can also train on lists of data. And we can also serialize it into a JSON. The above distributions were parametric, in that they were defined by a handful of parameters. Sometimes, this can lose the excentricities of the data. In contrast, nonparametric distributions require only data, not parameters. We can see this clearly below, where we have some points drawn from the unit norm distribution, and a few outliers. We can see that the Gaussian Kernel Densities can be especially useful in modelling outliers without having to come up with complicated distributions. A downside is that calculating log probabilities takes time proportional to the number of points in the kernel density, as opposed to constant time. It ' s much like kNN in that it takes  O ( 1 ) O(1)  to train, and  O ( n ) O(n)  to predict. Even with intermediate state caching, the Gaussian kernel density is ~5x slower on only 12 points. This is even after a recent update where I made them  > 30x faster than they used to be. However, it is true that kernel densities do have a parameter, which is the bandwidth. For Gaussian kernel densities, it is similar to adjusting the variance in the Gaussian distribution. Lets take a look: We can clearly see now that the outliers are dragging the normal distribution to the right, but the kernel density remains unchanged. We can look at a slightly more extreme example to see this truly. This kernel density can thus capture heavy tailed distributions without having an unnecessarily skewed variance. Lastly, you can also assign weights to all of the points if you choose to. We can see what this looks like when we highly weight the outliers. Sometimes, a univariate distribution isn't good enough. Many times data comes in the form of multiple, independent, dimensions, each with their own distribution. We can model this with a multivariate distribution which contains a tuple of independent components. A common example may be to look at data whose signal is normally distributed, and whose duration is modelled by an exponential. We can also weight the contributions of each distribution. In this example, lets say that the signal is far more important than the duration. We see that the point becomes more probable, because we care more about the good fit to the signal dimension (the normal distribution) than it poorly fits the duration (exponential) distribution. You must be careful with the weightings though, because they don't have to sum to 1. In this sense, weights = [3,1] is not the same as weights = [0.75, 0.25]. We can do updates in the same manner. Drawing sampels from a Gaussian with mu = 12 and sigma = 2, and an exponential with mu = 5 (0.2 since the inverse mu is more prominently used to parameterize an exponential), we can estimate the underlying distributions with 1000 points. It's not always good enough to have independent distributions, as signals can be heavily correlated. A common distribution for this is the multivariate gaussian distribution, utilizing a full covariance matrix. You provide a vector of mus, and a covariance matrix, and you're good to go! We can now train on data which has a diagonal covariance matrix, but whose mu's are now [0, 8, 16, 24, 32] instead of [0, 1, 2, 3, 4]. Lets see how well the model captures the underlying distribution. pomegranate is also faster than numpy in calculating these, due to an efficient cython implementation for calculating the covariance directly, instead of doing vector-matrix-vector multiplication, which allocates intermediate arrays. Given these tools, it's pretty simple to set up a Naive Bayes classifier using any combination of distributions you'd like. While not currently built in, you can set up the code in the following way: Looks like pomegranate is faster at Naive Bayes too, but lets make sure we've come to the same distribution in the end. It is frequently the case that the data you have is not explained by a single underlying distribution. If we want to try to recover the underlying distributions, we need to have a model which has multiple components. An example is the following data, which is clearly two gaussian distributions, but with different parameters. We must start with an initial estimate of what this distribution is. In this case, we would likely initialize our General Mixture Model to be of two Gaussian distributions, one centered around 2 with a unit variance, and the other centered around 8 with a unit variance. It looks like there are more points in the distribution to the right, so maybe we want to start off with a higher weight on that distribution. Before we do any training, lets take a look at what the model predicts. All models have a  fit  and a  predict  method, in keeping with the scikit-learn API. In this case, we want to predict each point and see which component it falls under. We started off with 2000 1 labels and 1000 0 labels, so this is somewhat close. It's a decent first look. But we can go on to visualize this to get a better look. This looks okay. We see there is a clear linear boundary between the two. This makes sense, because if you have two Gaussians of any dimensionality, the best separator is just a hyperplane. This is one of the reasons that Naive Bayes and Logistic Regresson can work very well on Gaussian distributed data. However, we are using  probabilistic  models here. We should take advantage of this, and predict the probability that each point belongs to each distribution, in order to get softer estimates. In keeping with the scikit-learn API, if we want probabilistic measurements we use the  predict_proba  method, and columns become each component and rows become each sample. Looks like we get ~1% closer. This is not terribly significant, but useful when we do training, which uses the expectation-maximization (EM) algorithm. I won't go into the details here, but the gist is that we want to train this distribution in an unsupervised fashion, and so we iterate between expectation (calculating probabilities of each point given the data) and maximization (updating model parameters given these probabilities). We iterate between those two steps until convergence. Training the models exemplifies why sometimes we want to use soft classification. The difference between 981 and 944 is significant, particularly in fuzzy cases where it's not always clear what's going on. As a continued theme, it allows us to bring our uncertainty along with us, and make more informed decisions because of this. However, a more common type of mixture model is the Gaussian Mixture Model, commonly confused with the General Mixture Model because they share similar names. However, the General Mixture Model can be a mixture of any number of distributions of heterogeneous types. Lets take a look at a Gaussian Mixture Model using MultivariateGaussianDistributions. pomegranate is several times faster than scikit-learn in this case, though scikit-learn does offer more fully-featured GMMs (such as DPGMM and tied covariance matrices), and it is a bit easier to initialize a GMM. Hidden Markov models (HMMs) are the flagship of the pomegranate package, in that most time is spent improving their implementation, and these improvements sometimes trickle down into the other algorithms. Lets delve into the features which pomegranate offers. Hidden Markov models are a form of structured prediction method which extend general mixture models to sequences of data, where position in the sequence is relevant. If each point in this sequence is completely independent of the other points, then HMMs are not the right tools and GMMs (or more complicated Bayesian networks) may be a better tool. The most common examples of HMMs come from bioinformatics and natural language processing. Since I am a bioinformatician, I will predominately use examples from bioinformatics. Lets take the simplified example of CG island detection on a sequence of DNA. DNA is made up of the four canonical nucleotides, abbreviated 'A', 'C', 'G', and 'T'. Specific organizations of these nucleotides encode enough information to build you, a human being. One simple region in the genome is called the 'CG' island, where the nucleotides 'C' and 'G' are enriched. Lets compare the predictions of a GMM with the predictions of a HMM, to both understand conceptually the differences between the two, and to see how easy it is to use pomegranate. Note: The HMM and GMM predictions may be the inverse of each other, because HMM states undergo a topological sort in order to properly handle silent states (more later), which can change from the order they were inserted into the model. At this point, a dense HMM with equal probabilities between each state is ~equivalent~ to a GMM. However, this framework gives us great flexibility to add prior knowledge through edge transition probabilities, whereas a GMM doesn't. If we look at the predictions, we see that it's bifurcating between "background" and "CG island" very quickly--in essence, calling every C or G a 'CG island'. This is not likely to be true. We know that CG islands have some As and Ts in them, and background sequence has Cs and Gs. We can change the transition probabilities to account for this, and prevent switching from occuring too rapidly. This seems far more reasonable. There is a single CG island surrounded by background sequence, and something at the end. If we knew that CG islands cannot occur at the end of sequences, we need only modify the underlying structure of the HMM in order to say that the sequence must end from the background state. Looks like we managed to get rid of that pesky end (again, the numbers may have flipped, look at the indices). Modifying transition probabilities and using non-dense graphical structures are two major ways in which HMMs account for data in a sequence not being independent and identically distributed (i.i.d.). In fact, in most applications, the graphical structure of a HMM is very sparse. If we want a more probabilistic view of what's going on, we can get the probability of each symbol in the sequence being in each of the states in the model easily. This is useful to get a soft estimate of classification, which allows us to include confidence as well as prediction. Values close to 50-50 get masked when you make hard classifications, but this uncertainty can be passed to future applications if you use soft assignments. Each row in the matrix is one symbol in the sequence, and the columns correspond to the two states identified above (CG island or background). There is a corresponding hmm.predict_log_proba method present if you want to get the log values. These are the emission probability values calculated by the forward backward algorithm, and can also be retrieved by calling hmm.forward_backward( seq ), which returns both the emission and the transition probability tables. Lets take a look at these tables! This is the transition table, which has the soft count of the number of transitions across an edge in the model given a single sequence. It is a square matrix of size equal to the number of states (including start and end state), with number of transitions from (row_id) to (column_id). This is exemplified by the 1.0 in the first row, indicating that there is one transition from background state to the end state, as that's the only way to reach the end state. However, the third (or fourth, depending on ordering) row is the transitions from the start state, and it only slightly favors the background state. These counts are not normalized to the length of the input sequence, but can easily be done so by dividing by row sums, column sums, or entire table sums, depending on your application. A possible reason not to normalize is to run several sequences through and add up their tables, because normalizing in the end and extracting some domain knowledge. It is extremely useful in practice. For example, we can see that there is an expectation of 2.8956 transitions from CG island to background, and 2.4 from background to CG island. This could be used to infer that there are ~2-3 edges, which makes sense if you consider that the start and end of the sequence seem like they might be part of the CG island states except for the strict transition probabilities used (look at the first few rows of the emission table above.) We've been using the forward backward algorithm and maximum a posteriori for decoding thus far, however maximum a posteriori decoding has the side effect that it is possible that it predicts impossible sequences in some edge cases. An alternative is Viterbi decoding, which at each step takes the most likely path, instead of sum of all paths, to produce hard assignments. We see here a case in which it does not do too well. The Viterbi path can be more conservative in its transitions due to the hard assignments it makes. In essence, if multiple possibile paths are possible at a given point, it takes the most likely path, even if the sum of all other paths is greater than the sum of that path. In problems with a lower signal to noise ratio, this can mask the signal. As a side note, we can use the following to get the maximum a posteriori and Viterbi paths: Lets move on to a more complicated structure, that of a profile HMM. A profile HMM is used to align a sequence to a reference 'profile', where the reference profile can either be a single sequence, or an alignment of many sequences (such as a reference genome). In essence, this profile has a 'match' state for every position in the reference profile, and 'insert' state, and a 'delete' state. The insert state allows the external sequence to have an insertion into the sequence without throwing off the entire alignment, such as the following: or a deletion, which is the opposite: The bars in the middle refer to a perfect match, whereas the lack of a bar means either a deletion/insertion, or a mismatch. A mismatch is where two positions are aligned together, but do not match. This models the biological phenomena of mutation, where one nucleotide can convert to another over time. It is usually more likely in biological sequences that this type of mutation occurs than that the nucleotide was deleted from the sequence (shifting all nucleotides over by one) and then another was inserted at the exact location (moving all nucleotides over again). Since we are using a probabilistic model, we get to define these probabilities through the use of distributions! If we want to model mismatches, we can just set our 'match' state to have an appropriate distribution with non-zero probabilities over mismatches. Lets now create a three nucleotide profile HMM, which models the sequence 'ACT'. We will fuzz this a little bit in the match states, pretending to have some prior information about what mutations occur at each position. If you don't have any information, setting a uniform, small, value over the other values is usually okay. The first and last sequence are entirely matches, meaning that it thinks the most likely alignment between the profile ACT and ACT is A-A, C-C, and T-T, which makes sense, and the most likely alignment between ACT and ACC is A-A, C-C, and T-C, which includes a mismatch. Essentially, it's more likely that there's a T-C mismatch at the end then that there was a deletion of a T at the end of the sequence, and a separate insertion of a C. The two middle sequences don't match very well, as expected! G's are not very likely in this profile at all. It predicts that the two G's are inserts, and that the C matches the C in the profile, before hitting the delete state because it can't emit a T. The third sequence thinks that the G is an insert, as expected, and then aligns the A and T in the sequence to the A and T in the master sequence, missing the middle C in the profile. By using deletes, we can handle other sequences which are shorter than three characters. Lets look at some more sequences of different lengths. Again, more of the same expected. You'll notice most of the use of insertion states are at I0, because most of the insertions are at the beginning of the sequence. It's more probable to simply stay in I0 at the beginning instead of go from I0 to D1 to I1, or going to another insert state along there. You'll see other insert states used when insertions occur in other places in the sequence, like 'ATTT' and 'ACGTG'. Now that we have the path, we need to convert it into an alignment, which is significantly more informative to look at. There are two main algorithms for training hidden Markov models-- Baum Welch (structured version of Expectation Maximization), and Viterbi training. Since we don't start off with labels on the data, these are both unsupervised training algorithms. In order to assign labels, Baum Welch uses EM to assign soft labels (weights in this case) to each point belonging to each state, and then using weighted MLE estimates to update the distributions. Viterbi assigns hard labels to each observation using the Viterbi algorithm, and then updates the distributions based on these hard labels. pomegranate is extremely well featured when it comes to regularization methods for training, supporting tied emissions and edges, edge and emission inertia, freezing nodes or edges, edge pseudocounts, and multithreaded training. See  this tutorial  for more information. Bayesian networks are a powerful probabilistic inference tool, in which a set of variables are represented as nodes, and the lack of an edge represents a conditional independence statement between them. Bayesian networks are powerful in their ability to infer the value of unknown variables given any combination of known variables. Obviously, as more is known about the system, the more accurate these inferences will be, but the same algorithm can handle very different amounts of information. While Bayesian networks can have extremely complex emission probabilities, usually Gaussian or conditional Gaussian distributions, pomegranate currently supports only discrete Bayesian networks. In order to do inference, factor graph belief propogation is used. If you didn't understand that, it's okay! Lets look at the simple Monty Hall example. The Monty Hall problem arose from the gameshow Let's Make a Deal, where a guest had to choose which one of three doors had a car behind it. The twist was that after the guest chose, the host, originally Monty Hall, would then open one of the doors the guest did not pick and ask if the guest wanted to switch which door they had picked. Initial inspection may lead you to believe that if there are only two doors left, there is a 50-50 chance of you picking the right one, and so there is no advantage one way or the other. However, it has been proven both through simulations and analytically that there is in fact a 66% chance of getting the prize if the guest switches their door, regardless of the door they initially went with. We can reproduce this result using Bayesian networks with three nodes, one for the guest, one for the prize, and one for the door Monty chooses to open. The door the guest initially chooses and the door the prize is behind are completely random processes across the three doors, but the door which Monty opens is dependent on both the door the guest chooses (it cannot be the door the guest chooses), and the door the prize is behind (it cannot be the door with the prize behind it). pomegranate uses loopy belief propogation on the factor graph to calculate marginals, meaning that it is an inexact algorithm, but converges to the exact solution on Bayesian networks which have a tree structure. We can use the predict_proba method in order to ask questions for single data points. As a baseline, lets see what happens if we don't put in any information. This should give us the marginal of the graph, which is that everything is equally likely. Now lets do something different, and say that the guest has chosen door 'A'. We do this by passing a dictionary to predict_proba with key pairs consisting of the name of the state (in the state object), and the value which that variable has taken. We can see that now Monty will not open door 'A', because the guest has chosen it. At the same time, the distribution over the prize has not changed, it is still equally likely that the prize is behind each door. Now, lets say that Monty opens door 'C' and see what happens. 