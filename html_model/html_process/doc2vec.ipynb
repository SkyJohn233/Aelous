{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-15 06:42:56,250 : INFO : 100 has loaded...\n",
      "2020-01-15 06:42:56,912 : INFO : 200 has loaded...\n",
      "2020-01-15 06:43:14,236 : INFO : 300 has loaded...\n",
      "2020-01-15 06:43:14,910 : INFO : 400 has loaded...\n",
      "2020-01-15 06:43:28,013 : INFO : 500 has loaded...\n",
      "2020-01-15 06:43:28,912 : INFO : 600 has loaded...\n",
      "2020-01-15 06:43:29,850 : INFO : 700 has loaded...\n",
      "2020-01-15 06:43:31,650 : INFO : 800 has loaded...\n",
      "2020-01-15 06:43:33,096 : INFO : 900 has loaded...\n",
      "2020-01-15 06:43:33,719 : INFO : 1000 has loaded...\n",
      "2020-01-15 06:43:36,022 : INFO : 1100 has loaded...\n",
      "2020-01-15 06:43:37,449 : INFO : 1200 has loaded...\n",
      "2020-01-15 06:43:46,083 : INFO : 1300 has loaded...\n",
      "2020-01-15 06:43:50,070 : INFO : 1400 has loaded...\n",
      "2020-01-15 06:43:50,587 : INFO : 1500 has loaded...\n",
      "2020-01-15 06:44:35,522 : INFO : 1600 has loaded...\n",
      "2020-01-15 06:44:36,326 : INFO : 1700 has loaded...\n",
      "2020-01-15 06:45:17,085 : INFO : 1800 has loaded...\n",
      "2020-01-15 06:45:18,219 : INFO : 1900 has loaded...\n",
      "2020-01-15 06:45:23,913 : INFO : 2000 has loaded...\n",
      "2020-01-15 06:45:25,332 : INFO : 2100 has loaded...\n",
      "2020-01-15 06:45:25,985 : INFO : 2200 has loaded...\n",
      "2020-01-15 06:45:29,812 : INFO : 2300 has loaded...\n",
      "2020-01-15 06:45:40,803 : INFO : 2400 has loaded...\n",
      "2020-01-15 06:45:43,619 : INFO : 2500 has loaded...\n",
      "2020-01-15 06:45:44,409 : INFO : 2600 has loaded...\n",
      "2020-01-15 06:45:54,303 : INFO : 2700 has loaded...\n",
      "2020-01-15 06:45:58,848 : INFO : 2800 has loaded...\n",
      "2020-01-15 06:46:01,521 : INFO : 2900 has loaded...\n",
      "2020-01-15 06:46:05,933 : INFO : 3000 has loaded...\n",
      "2020-01-15 06:46:07,019 : INFO : 3100 has loaded...\n",
      "2020-01-15 06:46:07,676 : INFO : 3200 has loaded...\n",
      "2020-01-15 06:46:08,341 : INFO : 3300 has loaded...\n",
      "2020-01-15 06:46:48,155 : INFO : 3400 has loaded...\n",
      "2020-01-15 06:46:59,110 : INFO : 3500 has loaded...\n",
      "2020-01-15 06:46:59,767 : INFO : 3600 has loaded...\n",
      "2020-01-15 06:47:00,945 : INFO : 3700 has loaded...\n",
      "2020-01-15 06:47:02,869 : INFO : 3800 has loaded...\n",
      "2020-01-15 06:47:03,846 : INFO : 3900 has loaded...\n",
      "2020-01-15 06:47:09,898 : INFO : 4000 has loaded...\n",
      "2020-01-15 06:47:15,248 : INFO : 4100 has loaded...\n",
      "2020-01-15 06:47:21,800 : INFO : 4200 has loaded...\n",
      "2020-01-15 06:47:23,467 : INFO : 4300 has loaded...\n",
      "2020-01-15 06:47:24,109 : INFO : 4400 has loaded...\n",
      "2020-01-15 06:47:25,244 : INFO : 4500 has loaded...\n",
      "2020-01-15 06:47:26,043 : INFO : 4600 has loaded...\n",
      "2020-01-15 06:47:26,767 : INFO : 4700 has loaded...\n",
      "2020-01-15 06:47:36,065 : INFO : 4800 has loaded...\n",
      "2020-01-15 06:47:36,681 : INFO : 4900 has loaded...\n",
      "2020-01-15 06:47:44,723 : INFO : 5000 has loaded...\n",
      "2020-01-15 06:47:45,820 : INFO : 5100 has loaded...\n",
      "2020-01-15 06:47:47,438 : INFO : 5200 has loaded...\n",
      "2020-01-15 06:47:48,564 : INFO : 5300 has loaded...\n",
      "2020-01-15 06:47:49,077 : INFO : 5400 has loaded...\n",
      "2020-01-15 06:47:50,428 : INFO : 5500 has loaded...\n",
      "2020-01-15 06:47:52,857 : INFO : 5600 has loaded...\n",
      "2020-01-15 06:47:53,987 : INFO : 5700 has loaded...\n",
      "2020-01-15 06:47:54,596 : INFO : 5800 has loaded...\n",
      "2020-01-15 06:48:07,315 : INFO : 5900 has loaded...\n",
      "2020-01-15 06:48:14,211 : INFO : 6000 has loaded...\n",
      "2020-01-15 06:48:16,931 : INFO : 6100 has loaded...\n",
      "2020-01-15 06:48:42,785 : INFO : 6200 has loaded...\n",
      "2020-01-15 06:48:43,426 : INFO : 6300 has loaded...\n",
      "2020-01-15 06:48:44,176 : INFO : 6400 has loaded...\n",
      "2020-01-15 06:48:47,647 : INFO : 6500 has loaded...\n",
      "2020-01-15 06:48:55,190 : INFO : 6600 has loaded...\n",
      "2020-01-15 06:48:55,677 : INFO : 6700 has loaded...\n",
      "2020-01-15 06:48:56,175 : INFO : 6800 has loaded...\n",
      "2020-01-15 06:48:57,075 : INFO : 6900 has loaded...\n",
      "2020-01-15 06:48:57,880 : INFO : 7000 has loaded...\n",
      "2020-01-15 06:48:58,581 : INFO : 7100 has loaded...\n",
      "2020-01-15 06:49:00,021 : INFO : 7200 has loaded...\n",
      "2020-01-15 06:49:00,769 : INFO : 7300 has loaded...\n",
      "2020-01-15 06:49:01,399 : INFO : 7400 has loaded...\n",
      "2020-01-15 06:49:02,501 : INFO : 7500 has loaded...\n",
      "2020-01-15 06:49:07,033 : INFO : 7600 has loaded...\n",
      "2020-01-15 06:49:13,552 : INFO : 7700 has loaded...\n",
      "2020-01-15 06:49:14,150 : INFO : 7800 has loaded...\n",
      "2020-01-15 06:49:17,163 : INFO : 7900 has loaded...\n",
      "2020-01-15 06:49:17,967 : INFO : 8000 has loaded...\n",
      "2020-01-15 06:49:18,972 : INFO : 8100 has loaded...\n",
      "2020-01-15 06:49:31,320 : INFO : 8200 has loaded...\n",
      "2020-01-15 06:49:42,183 : INFO : 8300 has loaded...\n",
      "2020-01-15 06:49:55,937 : INFO : 8400 has loaded...\n",
      "2020-01-15 06:49:59,581 : INFO : 8500 has loaded...\n",
      "2020-01-15 06:50:01,726 : INFO : 8600 has loaded...\n",
      "2020-01-15 06:50:03,571 : INFO : 8700 has loaded...\n",
      "2020-01-15 06:50:16,725 : INFO : 8800 has loaded...\n",
      "2020-01-15 06:50:17,526 : INFO : 8900 has loaded...\n",
      "2020-01-15 06:50:20,616 : INFO : 9000 has loaded...\n",
      "2020-01-15 06:50:32,089 : INFO : 9100 has loaded...\n",
      "2020-01-15 06:50:33,157 : INFO : 9200 has loaded...\n",
      "2020-01-15 06:50:37,606 : INFO : 9300 has loaded...\n",
      "2020-01-15 06:50:40,645 : INFO : 9400 has loaded...\n",
      "2020-01-15 06:50:43,031 : INFO : 9500 has loaded...\n",
      "2020-01-15 06:50:43,768 : INFO : 9600 has loaded...\n",
      "2020-01-15 06:51:26,682 : INFO : 9700 has loaded...\n",
      "2020-01-15 06:51:27,357 : INFO : 9800 has loaded...\n",
      "2020-01-15 06:51:29,464 : INFO : 9900 has loaded...\n",
      "2020-01-15 06:51:34,201 : INFO : 10000 has loaded...\n",
      "2020-01-15 06:51:42,562 : INFO : 10100 has loaded...\n",
      "2020-01-15 06:51:52,761 : INFO : 10200 has loaded...\n",
      "2020-01-15 06:51:53,259 : INFO : 10300 has loaded...\n",
      "2020-01-15 06:51:53,904 : INFO : 10400 has loaded...\n",
      "2020-01-15 06:52:04,516 : INFO : 10500 has loaded...\n",
      "2020-01-15 06:52:52,474 : INFO : 10600 has loaded...\n",
      "2020-01-15 06:52:53,128 : INFO : 10700 has loaded...\n",
      "2020-01-15 06:52:55,650 : INFO : 10800 has loaded...\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "documents = []\n",
    "count = 0\n",
    "input_dir = './processed_html/'\n",
    "output_dir = './tokenized_html/'\n",
    "english_punctuations = ['|', '-', '·', '``', ',', '.', ':', ';', '?', '(', ')', '[', ']', '<', '>', '&', '!', '*', '@', '#', '$', '%', '...', \"'\", '\"', \"--\", \"`\", \"“\", \"”\", \"’\", \"‘\", \"_\", \"''\", '+', '*', '/', '\\\\', '=', '~', '^', '{', '}']\n",
    "# file = 'page6171.txt'\n",
    "count = 0\n",
    "for file in os.listdir(input_dir):\n",
    "    with open(os.path.join(input_dir, file), 'r', encoding='utf-8') as f:\n",
    "        lines = []\n",
    "        for line in f.readlines():\n",
    "            # print(line)\n",
    "            line = nltk.word_tokenize(line)\n",
    "            # print(line)\n",
    "            line = [word.lower() for word in line if word not in english_punctuations]\n",
    "            # print(line)\n",
    "            lines.extend(line)\n",
    "        rec = open(os.path.join(output_dir, file), 'w', encoding='utf-8')\n",
    "        flag = False\n",
    "        for word in lines:\n",
    "            if flag:\n",
    "                rec.write(' ' + word)\n",
    "            else:\n",
    "                rec.write(word)\n",
    "                flag = True\n",
    "        rec.close()\n",
    "        documents.append(gensim.models.doc2vec.TaggedDocument(lines, [str(count)]))\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            logging.info('{} has loaded...'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10829\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./tokenized_html | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/usr/local/lib/python3.5/dist-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2020-01-15 06:59:26,203 : INFO : collecting all words and their counts\n",
      "2020-01-15 06:59:26,205 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-01-15 06:59:28,301 : INFO : PROGRESS: at example #10000, processed 7657892 words (3655466/s), 385705 word types, 10000 tags\n",
      "2020-01-15 06:59:28,447 : INFO : collected 405064 word types and 10829 unique tags from a corpus of 10829 examples and 8191789 words\n",
      "2020-01-15 06:59:28,448 : INFO : Loading a fresh vocabulary\n",
      "2020-01-15 06:59:29,297 : INFO : effective_min_count=5 retains 89547 unique words (22% of original 405064, drops 315517)\n",
      "2020-01-15 06:59:29,298 : INFO : effective_min_count=5 leaves 7701274 word corpus (94% of original 8191789, drops 490515)\n",
      "2020-01-15 06:59:29,596 : INFO : deleting the raw counts dictionary of 405064 items\n",
      "2020-01-15 06:59:29,620 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2020-01-15 06:59:29,621 : INFO : downsampling leaves estimated 6743911 word corpus (87.6% of prior 7701274)\n",
      "2020-01-15 06:59:30,032 : INFO : estimated required memory for 89547 words and 100 dimensions: 122908500 bytes\n",
      "2020-01-15 06:59:30,033 : INFO : resetting layer weights\n",
      "2020-01-15 06:59:31,267 : INFO : training model with 4 workers on 89547 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=8\n",
      "2020-01-15 06:59:32,274 : INFO : EPOCH 1 - PROGRESS: at 16.54% examples, 944878 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:33,287 : INFO : EPOCH 1 - PROGRESS: at 33.25% examples, 984259 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:34,294 : INFO : EPOCH 1 - PROGRESS: at 50.00% examples, 987467 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:35,297 : INFO : EPOCH 1 - PROGRESS: at 66.50% examples, 975589 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:36,306 : INFO : EPOCH 1 - PROGRESS: at 81.97% examples, 973897 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 06:59:37,307 : INFO : EPOCH 1 - PROGRESS: at 97.47% examples, 972931 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:37,414 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 06:59:37,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 06:59:37,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 06:59:37,435 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 06:59:37,436 : INFO : EPOCH - 1 : training on 8191789 raw words (6004022 effective words) took 6.2s, 973985 effective words/s\n",
      "2020-01-15 06:59:38,445 : INFO : EPOCH 2 - PROGRESS: at 16.34% examples, 926938 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:39,449 : INFO : EPOCH 2 - PROGRESS: at 31.68% examples, 950706 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:40,453 : INFO : EPOCH 2 - PROGRESS: at 47.70% examples, 950133 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:41,453 : INFO : EPOCH 2 - PROGRESS: at 65.44% examples, 958186 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:42,455 : INFO : EPOCH 2 - PROGRESS: at 80.36% examples, 957309 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:43,462 : INFO : EPOCH 2 - PROGRESS: at 96.01% examples, 961609 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:43,668 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 06:59:43,676 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 06:59:43,681 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 06:59:43,687 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 06:59:43,687 : INFO : EPOCH - 2 : training on 8191789 raw words (6004839 effective words) took 6.2s, 961177 effective words/s\n",
      "2020-01-15 06:59:44,701 : INFO : EPOCH 3 - PROGRESS: at 16.76% examples, 962557 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:45,705 : INFO : EPOCH 3 - PROGRESS: at 32.28% examples, 964863 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:46,708 : INFO : EPOCH 3 - PROGRESS: at 48.84% examples, 971899 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:47,713 : INFO : EPOCH 3 - PROGRESS: at 65.91% examples, 966632 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:48,715 : INFO : EPOCH 3 - PROGRESS: at 80.96% examples, 965392 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 06:59:49,719 : INFO : EPOCH 3 - PROGRESS: at 96.76% examples, 969256 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:49,863 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 06:59:49,873 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 06:59:49,876 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 06:59:49,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 06:59:49,881 : INFO : EPOCH - 3 : training on 8191789 raw words (6004239 effective words) took 6.2s, 970445 effective words/s\n",
      "2020-01-15 06:59:50,893 : INFO : EPOCH 4 - PROGRESS: at 16.71% examples, 952265 words/s, in_qsize 7, out_qsize 1\n",
      "2020-01-15 06:59:51,905 : INFO : EPOCH 4 - PROGRESS: at 33.81% examples, 996512 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:52,907 : INFO : EPOCH 4 - PROGRESS: at 50.84% examples, 1000998 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:53,908 : INFO : EPOCH 4 - PROGRESS: at 67.83% examples, 990818 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:54,925 : INFO : EPOCH 4 - PROGRESS: at 83.63% examples, 988477 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:55,933 : INFO : EPOCH 4 - PROGRESS: at 99.11% examples, 986302 words/s, in_qsize 6, out_qsize 0\n",
      "2020-01-15 06:59:55,947 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 06:59:55,955 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 06:59:55,960 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 06:59:55,965 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 06:59:55,966 : INFO : EPOCH - 4 : training on 8191789 raw words (6003708 effective words) took 6.1s, 987206 effective words/s\n",
      "2020-01-15 06:59:56,974 : INFO : EPOCH 5 - PROGRESS: at 16.80% examples, 964826 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 06:59:57,981 : INFO : EPOCH 5 - PROGRESS: at 32.69% examples, 972326 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 06:59:58,986 : INFO : EPOCH 5 - PROGRESS: at 49.36% examples, 979589 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 06:59:59,993 : INFO : EPOCH 5 - PROGRESS: at 66.50% examples, 976614 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:00,998 : INFO : EPOCH 5 - PROGRESS: at 82.01% examples, 976931 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:02,014 : INFO : EPOCH 5 - PROGRESS: at 98.49% examples, 981275 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:02,068 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:00:02,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:00:02,077 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:00:02,084 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:00:02,085 : INFO : EPOCH - 5 : training on 8191789 raw words (6005168 effective words) took 6.1s, 982116 effective words/s\n",
      "2020-01-15 07:00:03,096 : INFO : EPOCH 6 - PROGRESS: at 16.61% examples, 951659 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:04,100 : INFO : EPOCH 6 - PROGRESS: at 32.44% examples, 970488 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:05,116 : INFO : EPOCH 6 - PROGRESS: at 48.84% examples, 969193 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:06,126 : INFO : EPOCH 6 - PROGRESS: at 66.16% examples, 969028 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-15 07:00:07,128 : INFO : EPOCH 6 - PROGRESS: at 81.97% examples, 973536 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:08,131 : INFO : EPOCH 6 - PROGRESS: at 97.65% examples, 975272 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:08,231 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:00:08,238 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:00:08,242 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:00:08,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:00:08,249 : INFO : EPOCH - 6 : training on 8191789 raw words (6003944 effective words) took 6.2s, 975233 effective words/s\n",
      "2020-01-15 07:00:09,260 : INFO : EPOCH 7 - PROGRESS: at 16.70% examples, 955942 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:10,262 : INFO : EPOCH 7 - PROGRESS: at 33.23% examples, 986583 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:11,267 : INFO : EPOCH 7 - PROGRESS: at 51.24% examples, 1007321 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:12,270 : INFO : EPOCH 7 - PROGRESS: at 68.47% examples, 999887 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:13,271 : INFO : EPOCH 7 - PROGRESS: at 83.70% examples, 994641 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:14,247 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:00:14,254 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:00:14,255 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:00:14,265 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:00:14,266 : INFO : EPOCH - 7 : training on 8191789 raw words (6004605 effective words) took 6.0s, 998585 effective words/s\n",
      "2020-01-15 07:00:15,277 : INFO : EPOCH 8 - PROGRESS: at 16.88% examples, 976188 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:16,288 : INFO : EPOCH 8 - PROGRESS: at 33.23% examples, 983994 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:17,289 : INFO : EPOCH 8 - PROGRESS: at 50.00% examples, 989098 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:18,291 : INFO : EPOCH 8 - PROGRESS: at 67.19% examples, 984858 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:19,296 : INFO : EPOCH 8 - PROGRESS: at 83.02% examples, 984916 words/s, in_qsize 6, out_qsize 1\n",
      "2020-01-15 07:00:20,309 : INFO : EPOCH 8 - PROGRESS: at 99.01% examples, 987367 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:20,334 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:00:20,340 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:00:20,341 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:00:20,349 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:00:20,350 : INFO : EPOCH - 8 : training on 8191789 raw words (6005037 effective words) took 6.1s, 987981 effective words/s\n",
      "2020-01-15 07:00:21,364 : INFO : EPOCH 9 - PROGRESS: at 17.07% examples, 985931 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:22,364 : INFO : EPOCH 9 - PROGRESS: at 33.11% examples, 982824 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:23,370 : INFO : EPOCH 9 - PROGRESS: at 49.36% examples, 978922 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:24,371 : INFO : EPOCH 9 - PROGRESS: at 66.85% examples, 979684 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:25,377 : INFO : EPOCH 9 - PROGRESS: at 82.01% examples, 977642 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:26,390 : INFO : EPOCH 9 - PROGRESS: at 98.49% examples, 982408 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:26,439 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:00:26,444 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:00:26,449 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:00:26,462 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:00:26,463 : INFO : EPOCH - 9 : training on 8191789 raw words (6005157 effective words) took 6.1s, 982813 effective words/s\n",
      "2020-01-15 07:00:27,471 : INFO : EPOCH 10 - PROGRESS: at 16.61% examples, 950783 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:28,472 : INFO : EPOCH 10 - PROGRESS: at 32.69% examples, 975015 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:29,473 : INFO : EPOCH 10 - PROGRESS: at 48.84% examples, 974647 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:00:30,473 : INFO : EPOCH 10 - PROGRESS: at 66.85% examples, 982457 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:31,475 : INFO : EPOCH 10 - PROGRESS: at 82.33% examples, 982272 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:00:32,480 : INFO : EPOCH 10 - PROGRESS: at 97.64% examples, 977918 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 07:00:32,585 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:00:32,593 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:00:32,594 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:00:32,602 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:00:32,602 : INFO : EPOCH - 10 : training on 8191789 raw words (6005529 effective words) took 6.1s, 978752 effective words/s\n",
      "2020-01-15 07:00:32,603 : INFO : training on a 81917890 raw words (60046248 effective words) took 61.3s, 978995 effective words/s\n",
      "2020-01-15 07:00:32,604 : INFO : saving Doc2Vec object under models/doc2vec1, separately None\n",
      "2020-01-15 07:00:33,744 : INFO : saved models/doc2vec1\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(documents, dm=1, size=100, window=8, min_count=5, workers=4, iter=10)\n",
    "model.save('models/doc2vec1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/usr/local/lib/python3.5/dist-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2020-01-15 07:02:36,085 : INFO : collecting all words and their counts\n",
      "2020-01-15 07:02:36,086 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-01-15 07:02:37,839 : INFO : PROGRESS: at example #10000, processed 7657892 words (4372135/s), 385705 word types, 10000 tags\n",
      "2020-01-15 07:02:37,965 : INFO : collected 405064 word types and 10829 unique tags from a corpus of 10829 examples and 8191789 words\n",
      "2020-01-15 07:02:37,966 : INFO : Loading a fresh vocabulary\n",
      "2020-01-15 07:02:39,420 : INFO : effective_min_count=2 retains 198175 unique words (48% of original 405064, drops 206889)\n",
      "2020-01-15 07:02:39,421 : INFO : effective_min_count=2 leaves 7984900 word corpus (97% of original 8191789, drops 206889)\n",
      "2020-01-15 07:02:40,073 : INFO : deleting the raw counts dictionary of 405064 items\n",
      "2020-01-15 07:02:40,094 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2020-01-15 07:02:40,095 : INFO : downsampling leaves estimated 7049636 word corpus (88.3% of prior 7984900)\n",
      "2020-01-15 07:02:41,029 : INFO : estimated required memory for 198175 words and 100 dimensions: 264124900 bytes\n",
      "2020-01-15 07:02:41,030 : INFO : resetting layer weights\n",
      "2020-01-15 07:02:43,693 : INFO : training model with 4 workers on 198175 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=8\n",
      "2020-01-15 07:02:44,704 : INFO : EPOCH 1 - PROGRESS: at 13.69% examples, 781341 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:02:45,705 : INFO : EPOCH 1 - PROGRESS: at 27.11% examples, 857536 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:02:46,714 : INFO : EPOCH 1 - PROGRESS: at 42.24% examples, 871471 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:02:47,724 : INFO : EPOCH 1 - PROGRESS: at 58.37% examples, 892527 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:02:48,742 : INFO : EPOCH 1 - PROGRESS: at 73.79% examples, 897581 words/s, in_qsize 7, out_qsize 1\n",
      "2020-01-15 07:02:49,744 : INFO : EPOCH 1 - PROGRESS: at 86.81% examples, 899299 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:02:50,609 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:02:50,622 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:02:50,625 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:02:50,634 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:02:50,635 : INFO : EPOCH - 1 : training on 8191789 raw words (6265210 effective words) took 6.9s, 903234 effective words/s\n",
      "2020-01-15 07:02:51,649 : INFO : EPOCH 2 - PROGRESS: at 15.05% examples, 876604 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:02:52,656 : INFO : EPOCH 2 - PROGRESS: at 28.80% examples, 908042 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:02:53,658 : INFO : EPOCH 2 - PROGRESS: at 43.60% examples, 896490 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:02:54,666 : INFO : EPOCH 2 - PROGRESS: at 58.81% examples, 900258 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:02:55,672 : INFO : EPOCH 2 - PROGRESS: at 74.06% examples, 902531 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:02:56,683 : INFO : EPOCH 2 - PROGRESS: at 87.63% examples, 905868 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:02:57,493 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:02:57,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:02:57,506 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:02:57,519 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:02:57,520 : INFO : EPOCH - 2 : training on 8191789 raw words (6264114 effective words) took 6.9s, 910250 effective words/s\n",
      "2020-01-15 07:02:58,535 : INFO : EPOCH 3 - PROGRESS: at 15.05% examples, 875107 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:02:59,540 : INFO : EPOCH 3 - PROGRESS: at 28.80% examples, 908232 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:00,541 : INFO : EPOCH 3 - PROGRESS: at 43.75% examples, 905842 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:01,549 : INFO : EPOCH 3 - PROGRESS: at 59.22% examples, 909417 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:02,552 : INFO : EPOCH 3 - PROGRESS: at 74.36% examples, 909151 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:03,562 : INFO : EPOCH 3 - PROGRESS: at 88.12% examples, 912751 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:04,342 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:04,353 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:04,355 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:04,366 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:04,367 : INFO : EPOCH - 3 : training on 8191789 raw words (6263225 effective words) took 6.8s, 915234 effective words/s\n",
      "2020-01-15 07:03:05,376 : INFO : EPOCH 4 - PROGRESS: at 15.35% examples, 899194 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:06,388 : INFO : EPOCH 4 - PROGRESS: at 29.63% examples, 931082 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:07,390 : INFO : EPOCH 4 - PROGRESS: at 45.06% examples, 935144 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:08,401 : INFO : EPOCH 4 - PROGRESS: at 60.45% examples, 932562 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:09,407 : INFO : EPOCH 4 - PROGRESS: at 75.56% examples, 926544 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:10,423 : INFO : EPOCH 4 - PROGRESS: at 89.49% examples, 928428 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:11,114 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:11,122 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:11,124 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:11,135 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:11,136 : INFO : EPOCH - 4 : training on 8191789 raw words (6263688 effective words) took 6.8s, 926481 effective words/s\n",
      "2020-01-15 07:03:12,151 : INFO : EPOCH 5 - PROGRESS: at 15.74% examples, 927105 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:13,154 : INFO : EPOCH 5 - PROGRESS: at 29.49% examples, 928880 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:14,156 : INFO : EPOCH 5 - PROGRESS: at 44.25% examples, 917135 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:15,167 : INFO : EPOCH 5 - PROGRESS: at 59.75% examples, 917595 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 07:03:16,173 : INFO : EPOCH 5 - PROGRESS: at 74.71% examples, 912585 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:17,174 : INFO : EPOCH 5 - PROGRESS: at 88.50% examples, 917047 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:17,968 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:17,975 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:17,978 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:17,989 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:17,989 : INFO : EPOCH - 5 : training on 8191789 raw words (6263903 effective words) took 6.8s, 915321 effective words/s\n",
      "2020-01-15 07:03:18,999 : INFO : EPOCH 6 - PROGRESS: at 15.44% examples, 906015 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:20,006 : INFO : EPOCH 6 - PROGRESS: at 28.80% examples, 911884 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:21,008 : INFO : EPOCH 6 - PROGRESS: at 43.89% examples, 910502 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:22,014 : INFO : EPOCH 6 - PROGRESS: at 59.50% examples, 915457 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-15 07:03:23,018 : INFO : EPOCH 6 - PROGRESS: at 74.34% examples, 907773 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 07:03:24,023 : INFO : EPOCH 6 - PROGRESS: at 88.06% examples, 913226 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:24,826 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:24,832 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:24,839 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:24,850 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:24,851 : INFO : EPOCH - 6 : training on 8191789 raw words (6264357 effective words) took 6.9s, 913949 effective words/s\n",
      "2020-01-15 07:03:25,856 : INFO : EPOCH 7 - PROGRESS: at 15.68% examples, 922370 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:26,864 : INFO : EPOCH 7 - PROGRESS: at 29.87% examples, 940302 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:27,871 : INFO : EPOCH 7 - PROGRESS: at 45.45% examples, 940033 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:28,890 : INFO : EPOCH 7 - PROGRESS: at 60.45% examples, 930553 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:29,896 : INFO : EPOCH 7 - PROGRESS: at 75.66% examples, 926281 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:30,910 : INFO : EPOCH 7 - PROGRESS: at 89.49% examples, 927490 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:31,608 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:31,613 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:31,621 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:31,633 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:31,634 : INFO : EPOCH - 7 : training on 8191789 raw words (6264825 effective words) took 6.8s, 924047 effective words/s\n",
      "2020-01-15 07:03:32,653 : INFO : EPOCH 8 - PROGRESS: at 15.51% examples, 901860 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:33,657 : INFO : EPOCH 8 - PROGRESS: at 28.97% examples, 911219 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:34,657 : INFO : EPOCH 8 - PROGRESS: at 44.25% examples, 914393 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:35,663 : INFO : EPOCH 8 - PROGRESS: at 59.22% examples, 909512 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:36,667 : INFO : EPOCH 8 - PROGRESS: at 74.85% examples, 915525 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:37,668 : INFO : EPOCH 8 - PROGRESS: at 88.13% examples, 914309 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:38,455 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:38,466 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:38,467 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:38,481 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:38,481 : INFO : EPOCH - 8 : training on 8191789 raw words (6264942 effective words) took 6.8s, 915415 effective words/s\n",
      "2020-01-15 07:03:39,499 : INFO : EPOCH 9 - PROGRESS: at 15.13% examples, 870782 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-15 07:03:40,500 : INFO : EPOCH 9 - PROGRESS: at 28.56% examples, 905085 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:41,501 : INFO : EPOCH 9 - PROGRESS: at 44.04% examples, 911243 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:42,510 : INFO : EPOCH 9 - PROGRESS: at 59.38% examples, 911484 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:43,511 : INFO : EPOCH 9 - PROGRESS: at 74.71% examples, 912650 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:44,519 : INFO : EPOCH 9 - PROGRESS: at 88.39% examples, 914999 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:45,254 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:45,263 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:45,264 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:45,277 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:45,278 : INFO : EPOCH - 9 : training on 8191789 raw words (6264276 effective words) took 6.8s, 922152 effective words/s\n",
      "2020-01-15 07:03:46,288 : INFO : EPOCH 10 - PROGRESS: at 15.92% examples, 932257 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:47,289 : INFO : EPOCH 10 - PROGRESS: at 30.53% examples, 956315 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-15 07:03:48,289 : INFO : EPOCH 10 - PROGRESS: at 45.06% examples, 937348 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:49,293 : INFO : EPOCH 10 - PROGRESS: at 60.39% examples, 933874 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:50,297 : INFO : EPOCH 10 - PROGRESS: at 75.55% examples, 927548 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:51,302 : INFO : EPOCH 10 - PROGRESS: at 89.13% examples, 927965 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-15 07:03:51,991 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-15 07:03:52,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-15 07:03:52,004 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-15 07:03:52,016 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-15 07:03:52,017 : INFO : EPOCH - 10 : training on 8191789 raw words (6264012 effective words) took 6.7s, 929973 effective words/s\n",
      "2020-01-15 07:03:52,017 : INFO : training on a 81917890 raw words (62642552 effective words) took 68.3s, 916865 effective words/s\n",
      "2020-01-15 07:03:52,074 : INFO : saving Doc2Vec object under models/doc2vec2, separately None\n",
      "2020-01-15 07:03:52,075 : INFO : storing np array 'vectors' to models/doc2vec2.wv.vectors.npy\n",
      "2020-01-15 07:03:52,133 : INFO : storing np array 'syn1neg' to models/doc2vec2.trainables.syn1neg.npy\n",
      "2020-01-15 07:03:52,847 : INFO : saved models/doc2vec2\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(documents, dm=1, size=100, window=8, min_count=2, workers=4, iter=10)\n",
    "model.save('models/doc2vec2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_doc2vec():\n",
    "    # 加载模型\n",
    "    model = Doc2Vec.load('models/doc2vec1')\n",
    "    # 与标签‘0’最相似的\n",
    "    print(model.docvecs.most_similar('6'))\n",
    "    # 进行相关性比较\n",
    "    print(model.docvecs.similarity('8','208'))\n",
    "    # 输出标签为‘10’句子的向量\n",
    "    print(model.docvecs['1000'])\n",
    "    # 也可以推断一个句向量(未出现在语料中)\n",
    "    words = u\"sailed boat\"\n",
    "    print(model.infer_vector(words.split()))\n",
    "    # 也可以输出词向量\n",
    "    print(model[u'sports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-15 07:11:01,730 : INFO : loading Doc2Vec object from models/doc2vec1\n",
      "2020-01-15 07:11:02,970 : INFO : loading wv recursively from models/doc2vec1.wv.* with mmap=None\n",
      "2020-01-15 07:11:02,975 : INFO : loading docvecs recursively from models/doc2vec1.docvecs.* with mmap=None\n",
      "2020-01-15 07:11:02,975 : INFO : loading vocabulary recursively from models/doc2vec1.vocabulary.* with mmap=None\n",
      "2020-01-15 07:11:02,976 : INFO : loading trainables recursively from models/doc2vec1.trainables.* with mmap=None\n",
      "2020-01-15 07:11:02,977 : INFO : loaded models/doc2vec1\n",
      "2020-01-15 07:11:03,216 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('5755', 0.9275752305984497), ('2453', 0.8274784088134766), ('1316', 0.8245440721511841), ('7262', 0.8228101134300232), ('71', 0.8223134279251099), ('3140', 0.8219643235206604), ('3438', 0.8215072154998779), ('746', 0.8198475241661072), ('9063', 0.8187520503997803), ('5659', 0.8174053430557251)]\n",
      "0.4759059\n",
      "[ 0.02491954 -0.13551797 -0.07276866 -0.00585771  0.04062539 -0.04910394\n",
      "  0.03813834  0.15348005  0.07990651  0.09032833 -0.04173096  0.04427738\n",
      "  0.01503846  0.17613368 -0.15115486  0.14723538  0.04658129  0.02665119\n",
      "  0.09300822 -0.12479114 -0.04141674 -0.02208107  0.17643099 -0.08007837\n",
      " -0.10159306 -0.15783367  0.03240332 -0.03601615  0.07183215 -0.05040471\n",
      "  0.0408824  -0.02976261  0.02507024 -0.00913375 -0.08721622 -0.13297746\n",
      " -0.00250732 -0.04867163 -0.08566206  0.00701508 -0.04643111 -0.02324632\n",
      "  0.17778854 -0.07284717  0.12037168  0.02691167 -0.0208843  -0.17849678\n",
      "  0.01731402  0.04851523 -0.06836087  0.02487659 -0.14963911  0.15890615\n",
      "  0.04214641 -0.03076054  0.05976051  0.10872677  0.1433736   0.01137837\n",
      "  0.00584402  0.004558   -0.02283164  0.15239263 -0.07359646  0.03196991\n",
      " -0.14977178 -0.00740801  0.03348907 -0.02790691 -0.06226692  0.05871446\n",
      " -0.05326479 -0.01080385  0.0422557  -0.1587776   0.05559877  0.12447097\n",
      "  0.00688705 -0.03053005 -0.05934    -0.0149721  -0.06758841 -0.03816172\n",
      "  0.05329144  0.11124241 -0.09541498  0.04693368  0.11182887  0.12162475\n",
      "  0.08051295 -0.03632072  0.06131117 -0.09503993  0.05050104 -0.01336336\n",
      " -0.03661682 -0.15964492  0.09076044  0.02780541]\n",
      "[-0.00046339 -0.07840826 -0.07500468  0.0296398   0.08283903 -0.00622447\n",
      "  0.02744638  0.05144464  0.00847926  0.05675659  0.01489788 -0.03978\n",
      "  0.0267536   0.09258519 -0.02459388  0.05500554  0.01100829 -0.00261028\n",
      " -0.03895399 -0.08963381 -0.03229947  0.03788514  0.06529976 -0.09438657\n",
      "  0.00128373  0.01702264  0.01198042 -0.09455097  0.05821047 -0.0376137\n",
      "  0.0110712  -0.00296141  0.00905551  0.02646097  0.05997797 -0.06760754\n",
      " -0.0370595   0.02638043 -0.04607532 -0.0465109  -0.03614991 -0.01228229\n",
      " -0.00757147 -0.05736947  0.07635315  0.0182962  -0.019653    0.00397265\n",
      "  0.06136884 -0.05145796  0.02987146  0.02671219 -0.08360326  0.00141414\n",
      " -0.01598688  0.00052764  0.02854195  0.02150711  0.03621695  0.00574967\n",
      " -0.05633916  0.06512384 -0.00517476  0.01462423  0.02203151 -0.02784298\n",
      " -0.12533623  0.01490416 -0.03743858  0.01609148 -0.04719887  0.02381865\n",
      " -0.02635749  0.01472657  0.04431158 -0.08279522  0.04875022  0.06035304\n",
      " -0.00149454  0.04809035 -0.03702471 -0.01333833 -0.07202462 -0.05656755\n",
      " -0.01451348  0.12936054  0.03724025  0.02462587 -0.03389589  0.07187893\n",
      "  0.03964977  0.01715721  0.03687867 -0.02584982  0.01448068  0.00394752\n",
      "  0.00284908 -0.04697492  0.07020314 -0.01919923]\n",
      "[-0.73641616  2.796723   -1.6805581  -0.275202    0.35741282  2.0424302\n",
      "  1.0361779   1.2457451   1.2886851   0.26286438 -1.8909477   0.42552724\n",
      "  3.7651565  -1.0583775   1.1626338   1.4557931  -1.8102474   0.975952\n",
      " -0.7291056  -2.3251762   0.8676507  -1.2371076   1.3516866  -3.5498047\n",
      " -0.9545241  -0.8586438  -2.7747326  -0.2122317   0.6561915   1.2250428\n",
      " -3.1913502   2.6306248   2.4684486  -0.87385297 -0.85855526 -1.3040936\n",
      " -0.2907579  -1.9130242  -1.0567156  -1.2670534   0.03421802  1.8914998\n",
      " -0.6686675  -0.8722241  -0.9643159  -2.0524356  -3.7368047  -1.8400702\n",
      "  2.8277354  -0.4364945   0.8880681   1.0389583  -1.3395504  -0.5072133\n",
      " -2.908307    0.5846288  -0.34290242 -0.7577243   0.20237333 -1.329566\n",
      " -0.7087543   0.33968914  1.1243125   0.01047367 -0.7949471   0.5053728\n",
      "  0.34158957 -0.14981009 -0.28219223 -3.510068   -2.1685421   1.4754163\n",
      "  1.0464323  -0.9218565   0.30188057 -0.61030763  0.95853263  0.65663296\n",
      " -1.4818805  -1.9390755   0.89790225  0.19340685  0.96431756  2.729973\n",
      " -4.0011883  -0.8189107  -1.6650413   1.0456154   0.3493386   0.7279544\n",
      "  0.7514764  -0.79280204 -1.6033078   0.9596691   1.4530541  -0.15031783\n",
      "  2.6454     -0.73812747  1.9714296   1.3676169 ]\n"
     ]
    }
   ],
   "source": [
    "test_doc2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-15 07:21:27,023 : INFO : loading Doc2Vec object from models/doc2vec1\n",
      "2020-01-15 07:21:27,911 : INFO : loading wv recursively from models/doc2vec1.wv.* with mmap=None\n",
      "2020-01-15 07:21:27,915 : INFO : loading docvecs recursively from models/doc2vec1.docvecs.* with mmap=None\n",
      "2020-01-15 07:21:27,916 : INFO : loading vocabulary recursively from models/doc2vec1.vocabulary.* with mmap=None\n",
      "2020-01-15 07:21:27,917 : INFO : loading trainables recursively from models/doc2vec1.trainables.* with mmap=None\n",
      "2020-01-15 07:21:27,918 : INFO : loaded models/doc2vec1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89547 word vectors.\n",
      "89548 100\n"
     ]
    }
   ],
   "source": [
    "# 导入预训练的词向量\n",
    "model_doc2vec = Doc2Vec.load('models/doc2vec1')\n",
    "word2idx = {\"_PAD\": 0}  # 初始化 `[word : index]` 字典\n",
    "vocab_list = [(k, model_doc2vec.wv[k]) for k, v in model_doc2vec.wv.vocab.items()]\n",
    "# 词向量矩阵\n",
    "embeddings_matrix = np.zeros((len(model_doc2vec.wv.vocab.items()) + 1, model_doc2vec.vector_size))\n",
    "print('Found %s word vectors.' % len(model_doc2vec.wv.vocab.items()))\n",
    "for i in range(len(vocab_list)):\n",
    "\tword = vocab_list[i][0]\n",
    "\tword2idx[word] = i + 1\n",
    "\tembeddings_matrix[i + 1] = vocab_list[i][1]\n",
    "maxlen = 300\n",
    "max_features = len(model_doc2vec.wv.vocab.items()) + 1\n",
    "embedding_dims = model_doc2vec.vector_size\n",
    "print(max_features, embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/49/cfb96f19d34f4bc3543a4bd0212cd146e38a5ad1ad4bbd653cfee8943bc1/tensorflow-2.1.0-cp35-cp35m-manylinux2010_x86_64.whl (421.8MB)\n",
      "\u001b[K     |████████████████████████████████| 421.8MB 39kB/s s eta 0:00:01     |██████████                      | 131.2MB 55.9MB/s eta 0:00:06     |█████████████                   | 172.2MB 49.5MB/s eta 0:00:06\n",
      "\u001b[?25hCollecting numpy<2.0,>=1.16.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e6/1715e592ef47f28f3f50065322423bb75619ed2f7c24be86380ecc93503c/numpy-1.18.1-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\n",
      "\u001b[K     |████████████████████████████████| 20.0MB 30.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.13.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.25.0)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/60/8cbf00c0deb50a971e6e3a015fb32513960a92867df979870a454481817c/scipy-1.4.1-cp35-cp35m-manylinux1_x86_64.whl (26.0MB)\n",
      "\u001b[K     |████████████████████████████████| 26.0MB 46.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Using cached https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/lib/python3/dist-packages (from tensorflow) (0.29.0)\n",
      "Collecting gast==0.2.2\n",
      "  Using cached https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.11.2)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.1.0)\n",
      "Collecting astor>=0.6.0\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.0.8)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Using cached https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.5/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.5/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.5/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.5/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.5/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.5/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (41.6.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.5/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.5/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.5/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.5/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
      "Building wheels for collected packages: opt-einsum, gast, absl-py, termcolor\n",
      "  Building wheel for opt-einsum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for opt-einsum: filename=opt_einsum-3.1.0-cp35-none-any.whl size=63907 sha256=f5e252d6097fd9529164471aa051e0aee680997bed49db246ce7a4bc88548f9f\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-cp35-none-any.whl size=7636 sha256=1b44cac8f10911ffa336f7ca65e32861b9a36ea92f647355795b945fc24eaa96\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-cp35-none-any.whl size=119399 sha256=c6d8ee53d0e068d6a230dde4228af4fabf0e20e7805e0d29eb90cbf1ee73b1dd\n",
      "  Stored in directory: /root/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-cp35-none-any.whl size=5680 sha256=b89120b714d72775dc547d96dfc1218ddb3b8f887bb02c14877f43203f67606f\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built opt-einsum gast absl-py termcolor\n",
      "\u001b[31mERROR: scikit-image 0.15.0 requires pillow>=4.3.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: imageio 2.6.1 requires pillow, which is not installed.\u001b[0m\n",
      "Installing collected packages: numpy, opt-einsum, scipy, tensorflow-estimator, gast, google-pasta, astor, absl-py, termcolor, tensorboard, tensorflow\n",
      "  Found existing installation: numpy 1.15.1\n",
      "    Uninstalling numpy-1.15.1:\n",
      "      Successfully uninstalled numpy-1.15.1\n",
      "  Found existing installation: scipy 1.1.0\n",
      "    Uninstalling scipy-1.1.0:\n",
      "      Successfully uninstalled scipy-1.1.0\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 gast-0.2.2 google-pasta-0.1.8 numpy-1.18.1 opt-einsum-3.1.0 scipy-1.4.1 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-15 07:44:59,205 : INFO : 100 has converted...\n",
      "2020-01-15 07:44:59,268 : INFO : 200 has converted...\n",
      "2020-01-15 07:44:59,461 : INFO : 300 has converted...\n",
      "2020-01-15 07:44:59,536 : INFO : 400 has converted...\n",
      "2020-01-15 07:44:59,609 : INFO : 500 has converted...\n",
      "2020-01-15 07:44:59,685 : INFO : 600 has converted...\n",
      "2020-01-15 07:44:59,760 : INFO : 700 has converted...\n",
      "2020-01-15 07:44:59,830 : INFO : 800 has converted...\n",
      "2020-01-15 07:44:59,910 : INFO : 900 has converted...\n",
      "2020-01-15 07:44:59,974 : INFO : 1000 has converted...\n",
      "2020-01-15 07:45:00,047 : INFO : 1100 has converted...\n",
      "2020-01-15 07:45:00,123 : INFO : 1200 has converted...\n",
      "2020-01-15 07:45:00,218 : INFO : 1300 has converted...\n",
      "2020-01-15 07:45:00,285 : INFO : 1400 has converted...\n",
      "2020-01-15 07:45:00,346 : INFO : 1500 has converted...\n",
      "2020-01-15 07:45:00,475 : INFO : 1600 has converted...\n",
      "2020-01-15 07:45:00,547 : INFO : 1700 has converted...\n",
      "2020-01-15 07:45:00,635 : INFO : 1800 has converted...\n",
      "2020-01-15 07:45:00,721 : INFO : 1900 has converted...\n",
      "2020-01-15 07:45:00,815 : INFO : 2000 has converted...\n",
      "2020-01-15 07:45:00,891 : INFO : 2100 has converted...\n",
      "2020-01-15 07:45:00,960 : INFO : 2200 has converted...\n",
      "2020-01-15 07:45:01,036 : INFO : 2300 has converted...\n",
      "2020-01-15 07:45:01,125 : INFO : 2400 has converted...\n",
      "2020-01-15 07:45:01,198 : INFO : 2500 has converted...\n",
      "2020-01-15 07:45:01,277 : INFO : 2600 has converted...\n",
      "2020-01-15 07:45:01,358 : INFO : 2700 has converted...\n",
      "2020-01-15 07:45:01,452 : INFO : 2800 has converted...\n",
      "2020-01-15 07:45:01,531 : INFO : 2900 has converted...\n",
      "2020-01-15 07:45:01,653 : INFO : 3000 has converted...\n",
      "2020-01-15 07:45:01,726 : INFO : 3100 has converted...\n",
      "2020-01-15 07:45:01,794 : INFO : 3200 has converted...\n",
      "2020-01-15 07:45:01,860 : INFO : 3300 has converted...\n",
      "2020-01-15 07:45:01,937 : INFO : 3400 has converted...\n",
      "2020-01-15 07:45:02,007 : INFO : 3500 has converted...\n",
      "2020-01-15 07:45:02,068 : INFO : 3600 has converted...\n",
      "2020-01-15 07:45:02,131 : INFO : 3700 has converted...\n",
      "2020-01-15 07:45:02,201 : INFO : 3800 has converted...\n",
      "2020-01-15 07:45:02,275 : INFO : 3900 has converted...\n",
      "2020-01-15 07:45:02,378 : INFO : 4000 has converted...\n",
      "2020-01-15 07:45:02,449 : INFO : 4100 has converted...\n",
      "2020-01-15 07:45:02,562 : INFO : 4200 has converted...\n",
      "2020-01-15 07:45:02,646 : INFO : 4300 has converted...\n",
      "2020-01-15 07:45:02,718 : INFO : 4400 has converted...\n",
      "2020-01-15 07:45:02,787 : INFO : 4500 has converted...\n",
      "2020-01-15 07:45:02,876 : INFO : 4600 has converted...\n",
      "2020-01-15 07:45:02,953 : INFO : 4700 has converted...\n",
      "2020-01-15 07:45:03,066 : INFO : 4800 has converted...\n",
      "2020-01-15 07:45:03,131 : INFO : 4900 has converted...\n",
      "2020-01-15 07:45:03,198 : INFO : 5000 has converted...\n",
      "2020-01-15 07:45:03,269 : INFO : 5100 has converted...\n",
      "2020-01-15 07:45:03,353 : INFO : 5200 has converted...\n",
      "2020-01-15 07:45:03,426 : INFO : 5300 has converted...\n",
      "2020-01-15 07:45:03,486 : INFO : 5400 has converted...\n",
      "2020-01-15 07:45:03,555 : INFO : 5500 has converted...\n",
      "2020-01-15 07:45:03,622 : INFO : 5600 has converted...\n",
      "2020-01-15 07:45:03,678 : INFO : 5700 has converted...\n",
      "2020-01-15 07:45:03,746 : INFO : 5800 has converted...\n",
      "2020-01-15 07:45:03,842 : INFO : 5900 has converted...\n",
      "2020-01-15 07:45:03,920 : INFO : 6000 has converted...\n",
      "2020-01-15 07:45:03,999 : INFO : 6100 has converted...\n",
      "2020-01-15 07:45:04,072 : INFO : 6200 has converted...\n",
      "2020-01-15 07:45:04,151 : INFO : 6300 has converted...\n",
      "2020-01-15 07:45:04,227 : INFO : 6400 has converted...\n",
      "2020-01-15 07:45:04,317 : INFO : 6500 has converted...\n",
      "2020-01-15 07:45:04,412 : INFO : 6600 has converted...\n",
      "2020-01-15 07:45:04,479 : INFO : 6700 has converted...\n",
      "2020-01-15 07:45:04,545 : INFO : 6800 has converted...\n",
      "2020-01-15 07:45:04,625 : INFO : 6900 has converted...\n",
      "2020-01-15 07:45:04,707 : INFO : 7000 has converted...\n",
      "2020-01-15 07:45:04,780 : INFO : 7100 has converted...\n",
      "2020-01-15 07:45:04,878 : INFO : 7200 has converted...\n",
      "2020-01-15 07:45:04,952 : INFO : 7300 has converted...\n",
      "2020-01-15 07:45:05,012 : INFO : 7400 has converted...\n",
      "2020-01-15 07:45:05,088 : INFO : 7500 has converted...\n",
      "2020-01-15 07:45:05,194 : INFO : 7600 has converted...\n",
      "2020-01-15 07:45:05,267 : INFO : 7700 has converted...\n",
      "2020-01-15 07:45:05,333 : INFO : 7800 has converted...\n",
      "2020-01-15 07:45:05,420 : INFO : 7900 has converted...\n",
      "2020-01-15 07:45:05,491 : INFO : 8000 has converted...\n",
      "2020-01-15 07:45:05,567 : INFO : 8100 has converted...\n",
      "2020-01-15 07:45:05,659 : INFO : 8200 has converted...\n",
      "2020-01-15 07:45:05,739 : INFO : 8300 has converted...\n",
      "2020-01-15 07:45:05,826 : INFO : 8400 has converted...\n",
      "2020-01-15 07:45:05,896 : INFO : 8500 has converted...\n",
      "2020-01-15 07:45:05,985 : INFO : 8600 has converted...\n",
      "2020-01-15 07:45:06,079 : INFO : 8700 has converted...\n",
      "2020-01-15 07:45:06,179 : INFO : 8800 has converted...\n",
      "2020-01-15 07:45:06,258 : INFO : 8900 has converted...\n",
      "2020-01-15 07:45:06,332 : INFO : 9000 has converted...\n",
      "2020-01-15 07:45:06,410 : INFO : 9100 has converted...\n",
      "2020-01-15 07:45:06,492 : INFO : 9200 has converted...\n",
      "2020-01-15 07:45:06,568 : INFO : 9300 has converted...\n",
      "2020-01-15 07:45:06,656 : INFO : 9400 has converted...\n",
      "2020-01-15 07:45:06,724 : INFO : 9500 has converted...\n",
      "2020-01-15 07:45:06,789 : INFO : 9600 has converted...\n",
      "2020-01-15 07:45:06,877 : INFO : 9700 has converted...\n",
      "2020-01-15 07:45:06,942 : INFO : 9800 has converted...\n",
      "2020-01-15 07:45:07,041 : INFO : 9900 has converted...\n",
      "2020-01-15 07:45:07,130 : INFO : 10000 has converted...\n",
      "2020-01-15 07:45:07,213 : INFO : 10100 has converted...\n",
      "2020-01-15 07:45:07,295 : INFO : 10200 has converted...\n",
      "2020-01-15 07:45:07,354 : INFO : 10300 has converted...\n",
      "2020-01-15 07:45:07,421 : INFO : 10400 has converted...\n",
      "2020-01-15 07:45:07,500 : INFO : 10500 has converted...\n",
      "2020-01-15 07:45:07,575 : INFO : 10600 has converted...\n",
      "2020-01-15 07:45:07,640 : INFO : 10700 has converted...\n",
      "2020-01-15 07:45:07,707 : INFO : 10800 has converted...\n"
     ]
    }
   ],
   "source": [
    "# 读入预处理后的文本，根据预训练的词向量字典构建句子序列\n",
    "# x = []\n",
    "output_dir2 = './vectors/'\n",
    "# file = 'page10000.txt'\n",
    "count = 0\n",
    "for file in os.listdir(output_dir):\n",
    "    count += 1\n",
    "    with open(os.path.join(output_dir, file), 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            this_x = []\n",
    "            words = line.split(' ')\n",
    "            for word in words:\n",
    "                if word in word2idx.keys():\n",
    "                    this_x.append(word2idx[word])\n",
    "        # x.append(this_x)\n",
    "\n",
    "        npy_name = file[:-3]\n",
    "        npy_name = output_dir2 + npy_name + 'npy'\n",
    "        # print(npy_name)\n",
    "        # 保存\n",
    "        np.save(npy_name, this_x)   # 保存为.npy格式\n",
    "        # print(this_x)\n",
    "        if count % 100 == 0:\n",
    "            logging.info('{} has converted...'.format(count))\n",
    "\n",
    "# x = sequence.pad_sequences(x, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26217, 42923, 71460, 39479, 2746, 61871, 52787, 83276, 76785, 920, 20917, 68060, 46678, 11211, 5066, 39479, 65037, 25246, 28582, 41672, 61871, 78823, 39479, 37872, 12291, 50627, 71986, 30077, 77598, 49810, 41349, 38197, 58647, 61871, 88231, 46410, 25246, 28582, 41672, 79550, 88231, 37764, 42435, 46410, 17016, 46498, 39479, 49193, 47623, 23529, 38197, 82503, 63803, 37278, 88231, 46410, 25246, 28582, 41672, 79550, 30373, 11009, 62638, 42473, 88231, 30869, 67938, 64873, 46410, 77598, 17016, 73234, 67344, 71986, 9547, 69294, 82503, 28582, 73669, 38988, 67765, 49810, 86356, 61544, 88231, 39016, 28582, 39384, 61871, 11786, 54278, 71986, 39479, 12229, 28582, 13401, 14021, 77598, 47574, 38795, 37872, 46410, 41567, 25246, 28582, 41672, 87228, 8028, 73702, 49810, 48226, 39508, 61871, 14783, 38197, 29945, 47574, 86356, 73189, 11786, 48049, 13572, 64069, 77598, 57754, 32097, 21488, 83708, 42353, 60733, 46410, 37764, 42435, 55840, 85537, 43670, 49810, 67240, 39508, 48562, 14962, 67344, 71460, 11879, 26286, 57516, 61231, 5697, 88308, 28582, 42098, 39479, 47713, 64069, 44941, 46410, 40846, 39508, 46410, 17016, 36571, 46410, 38025, 61871, 42923, 30967, 39479, 49193, 46814, 37872, 12398, 39508, 87956, 11211, 71986, 80693, 71548, 81089, 88231, 61801, 42435, 64069, 44941, 46410, 40846, 38197, 17016, 46498, 46410, 87004, 64588, 37872, 49810, 69905, 48049, 46410, 41349, 86723, 38025, 46049, 47758, 37872, 31298, 46410, 21047, 75103, 39479, 49193, 52848, 57447, 46814, 54658, 69294, 7088, 39508, 87956, 37395, 12057, 28582, 29548, 72122, 8581, 60877, 71986, 82885, 49193, 46410, 41349, 37335, 57774, 87956, 45674, 72122, 86721, 88231, 1170, 57447, 71986, 21225, 16957, 37872, 49810, 17016, 38197, 84560, 87331, 22173, 7088, 83276, 76785, 26432, 920, 20917, 37872, 57888, 83276, 76785, 26432, 42473, 62638, 46410, 63681, 39508, 83276, 76785, 26432, 16109, 38197, 11009, 20836, 44245, 62394, 87117, 28582, 54251, 46410, 15361, 89360, 19202, 20222, 20917, 83276, 76785, 26432, 42473, 73234, 46410, 15677, 59786, 37872, 83276, 76785, 26432, 16109, 28582, 34601, 73234, 46410, 12448, 28582, 12229, 39508, 83276, 76785, 26432, 42473, 83276, 76785, 26432, 42473, 81204, 42435, 54066, 49810, 56904, 88231, 46410, 42435, 63681, 39508, 46410, 41349, 11009, 20836, 44245, 62394, 87117, 46410, 41349, 80240, 46410, 51177, 64873, 78668, 56904, 71986, 45058, 59786, 31030, 77681, 38659, 28582, 38197, 52926, 42156, 31030, 77681, 78591, 79813, 57225, 42435, 63681, 37872, 46410, 77681, 38659, 52926, 68107, 71986, 32713, 83276, 76785, 26432, 42473, 71986, 64684, 82026, 44941, 46410, 51177, 49465, 85537, 46410, 22601, 63681, 38197, 48868, 44245, 36671, 57888, 50620, 83276, 76785, 26432, 16109, 24268, 27248, 13734, 39508, 42435, 82026, 15056, 40402, 52156, 72592, 46410, 41349, 80074, 13734, 39508, 42435, 37872, 46410, 52674, 33234, 69294, 7088, 10975, 46498, 49810, 8954, 40402, 75255, 52156, 46410, 52674, 33234, 39508, 27248, 83276, 76785, 76786, 28582, 24836, 46298, 71885, 42841, 71986, 46410, 7205, 39508, 62002, 74159, 25246, 29484, 55530, 74159, 29484, 28582, 65643, 25246, 54278, 71986, 87956, 11394, 46410, 41349, 57754, 7205, 79681, 31030, 46410, 56068, 14687, 47098, 14687, 28582, 37872, 46410, 86947, 7031, 49810, 42923, 35485, 29512, 80984, 77241, 71986, 9547, 32834, 79769, 39405, 11211, 78337, 71403, 62201, 64954, 83261, 39508, 38104, 13919, 31030, 71072, 30169, 39508, 69294, 25064, 77121, 61870, 39508, 47574, 60074, 42234, 337, 60735, 28582, 45300, 40192, 28582, 9077, 13452, 337, 60735, 46410, 61795, 31030, 69294, 25064, 31489, 88107, 11397, 20571, 21674, 14972, 7935, 54066, 8270, 3130, 13609, 66667, 46410, 14741, 5875, 55735, 39508, 79769, 39405, 11211, 78337]\n"
     ]
    }
   ],
   "source": [
    "# 读取\n",
    "a=np.load('./vectors/page10000.npy')\n",
    "a = a.tolist()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10829\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./vectors | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
